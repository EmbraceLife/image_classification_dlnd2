{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewrite dlnd_project2 in kur\n",
    "- [original project_2 code](http://nbviewer.jupyter.org/github/EmbraceLife/image_classification_dlnd2/blob/master/cnn_cifar10.ipynb#annotations:5YK7tv1dEealzwMjXZx4WQ) in tensorflow\n",
    "- [detailed outline](https://github.com/EmbraceLife/image_classification_dlnd2/blob/master/outline.md) of the original code\n",
    "- kur code to replicate the original code is below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kur, by Deepgram -- deep learning made easy\r\n",
      "Version: 0.3.0\r\n",
      "Homepage: https://kur.deepgram.com\r\n"
     ]
    }
   ],
   "source": [
    "!kur --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dlnd_p2.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile dlnd_p2.yml\n",
    "\n",
    "---\n",
    "settings:\n",
    "\n",
    "  # Where to get the data\n",
    "  cifar: &cifar\n",
    "    url: \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "    checksum: \"6d958be074577803d12ecdefd02955f39262c83c16fe9348329d7fe0b5c001ce\"\n",
    "    path: \"~/kur\"           # if kur does not have normalization or one-hot-encoding, ##################\n",
    "                            # without normalization and one-hot-encoding, the performance will be hurt, right? ####\n",
    "  # Backend to use                                      \n",
    "  backend:\n",
    "    name: keras\n",
    "\n",
    "  # Hyperparameters\n",
    "  cnn:\n",
    "    kernels: 20\n",
    "    size: [5, 5]\n",
    "    strides: [2, 2]                \n",
    "\n",
    "  pool:\n",
    "    size: [2,2]\n",
    "    strides: [2,2]\n",
    "    type: \"max\"                         # must use a string here, {max} won't work, doc didn't say it #############\n",
    "\n",
    "model:\n",
    "  - input: images                       # images are normalized from 0 to 1, labels are one-hot-encoding ##########\n",
    "  - convolution:                        # does kur do normalize and one-hot-encoding under the hood? ##############\n",
    "      kernels: \"{{cnn.kernels}}\"\n",
    "      size: \"{{cnn.size}}\"\n",
    "      strides: \"{{ cnn.strides }}\"\n",
    "  - activation: relu\n",
    "  - pool:\n",
    "      size: \"{{pool.size}}\"\n",
    "      strides: \"{{pool.strides}}\"\n",
    "      type: \"{{pool.type}}\"             \n",
    "  - flatten:\n",
    "  - dense: 15                           # p2 want a dropout applied here, but kur does not have yet?? ############\n",
    "#   - dropout: 0.75\n",
    "  - dense: 10\n",
    "  - activation: softmax\n",
    "    name: labels\n",
    "\n",
    "train:\n",
    "  data:\n",
    "    - cifar:\n",
    "        <<: *cifar\n",
    "        parts: [1]                      # only use dataset part 1 to train\n",
    "  provider:\n",
    "    batch_size: 128\n",
    "    num_batches: 1000                   # the entire part 1 will be used\n",
    "  log: t3/cifar-log                   \n",
    "  epochs: 20\n",
    "  weights:\n",
    "    initial: t3/cifar.best.valid.w    \n",
    "    best: t3/cifar.best.train.w\n",
    "    last: t3/cifar.last.w\n",
    "\n",
    "  optimizer:\n",
    "    name: adam\n",
    "    learning_rate: 0.001\n",
    "\n",
    "validate:\n",
    "  data:\n",
    "    - cifar:\n",
    "       <<: *cifar\n",
    "       parts: 5                          # only use dataset part 5 as validation set\n",
    "  provider:\n",
    "    num_batches: 50                      # the project 2 only used 5000 data points as validation set\n",
    "  weights: t3/cifar.best.valid.w\n",
    "\n",
    "test: &test\n",
    "  data:\n",
    "    - cifar:\n",
    "       <<: *cifar\n",
    "       parts: test\n",
    "  weights: t3/cifar.best.valid.w\n",
    "  provider:\n",
    "    num_batches: 1000                     # the entire part test will be used\n",
    "\n",
    "evaluate:\n",
    "  <<: *test\n",
    "  destination: t3/cifar.results.pkl\n",
    "\n",
    "loss:\n",
    "  - target: labels                        # in the project: training loss and valid_accuracy are printed #############\n",
    "    name: categorical_crossentropy        # this should be a matter of personal taste, won't really affect anything##\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/Natsume/Downloads/kur_road'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kur_Road.ipynb       dlnd_p2_dropout.yml  \u001b[34mt1\u001b[m\u001b[m/\r\n",
      "\u001b[34mcifar-log\u001b[m\u001b[m/           dlnd_p2_kur.ipynb    \u001b[34mt2\u001b[m\u001b[m/\r\n",
      "dlnd_p2.yml          \u001b[34mkur\u001b[m\u001b[m/                 \u001b[34mt3\u001b[m\u001b[m/\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;37m[INFO 2017-02-28 22:28:48,972 kur.kurfile:638]\u001b[0m Parsing source: dlnd_p2.yml, included by top-level.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:28:48,989 kur.kurfile:79]\u001b[0m Parsing Kurfile...\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:28:49,038 kur.loggers.binary_logger:87]\u001b[0m Log does not exist. Creating path: t3/cifar-log\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:28:56,014 kur.providers.batch_provider:54]\u001b[0m Batch size set to: 128\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:28:56,014 kur.providers.batch_provider:60]\u001b[0m Maximum number of batches set to: 1000\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:02,413 kur.providers.batch_provider:54]\u001b[0m Batch size set to: 32\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:02,413 kur.providers.batch_provider:60]\u001b[0m Maximum number of batches set to: 50\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:02,414 kur.backend.backend:80]\u001b[0m Creating backend: keras\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:02,414 kur.backend.backend:83]\u001b[0m Backend variants: none\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:02,414 kur.backend.keras_backend:122]\u001b[0m No particular backend for Keras has been requested.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:03,578 kur.backend.keras_backend:191]\u001b[0m Keras is loaded. The backend is: theano\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:03,579 kur.model.model:260]\u001b[0m Enumerating the model containers.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:03,579 kur.model.model:265]\u001b[0m Assembling the model dependency graph.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:03,579 kur.model.model:280]\u001b[0m Connecting the model graph.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:04,579 kur.model.model:284]\u001b[0m Model inputs:  images\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:04,579 kur.model.model:285]\u001b[0m Model outputs: labels\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:04,580 kur.kurfile:310]\u001b[0m Ignoring missing initial weights: t3/cifar.best.valid.w. If this is undesireable, set \"must_exist\" to \"yes\" in the approriate \"weights\" section.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:05,910 kur.providers.batch_provider:54]\u001b[0m Batch size set to: 2\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:05,910 kur.providers.batch_provider:60]\u001b[0m Maximum number of batches set to: 1\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:05,913 kur.backend.keras_backend:654]\u001b[0m Waiting for model to finish compiling...\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:05,956 kur.model.executor:256]\u001b[0m No historical training loss available from logs.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:05,956 kur.model.executor:264]\u001b[0m No historical validation loss available from logs.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:05,956 kur.model.executor:270]\u001b[0m No previous epochs.\u001b[0m\n",
      "\n",
      "Epoch 1/20, loss=2.035: 100%|██████| 10000/10000 [00:02<00:00, 3344.74samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:08,954 kur.model.executor:390]\u001b[0m Training loss: 2.035\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:08,954 kur.model.executor:397]\u001b[0m Saving best historical training weights: t3/cifar.best.train.w\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:09,202 kur.providers.batch_provider:54]\u001b[0m Batch size set to: 2\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:09,203 kur.providers.batch_provider:60]\u001b[0m Maximum number of batches set to: 1\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:09,208 kur.backend.keras_backend:654]\u001b[0m Waiting for model to finish compiling...\u001b[0m\n",
      "Validating, loss=1.854: 100%|████████| 1600/1600 [00:00<00:00, 4341.85samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:09,585 kur.model.executor:175]\u001b[0m Validation loss: 1.854\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:09,585 kur.model.executor:422]\u001b[0m Saving best historical validation weights: t3/cifar.best.valid.w\u001b[0m\n",
      "\n",
      "Epoch 2/20, loss=1.693: 100%|██████| 10000/10000 [00:02<00:00, 4339.70samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:11,897 kur.model.executor:390]\u001b[0m Training loss: 1.693\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:11,897 kur.model.executor:397]\u001b[0m Saving best historical training weights: t3/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.641: 100%|████████| 1600/1600 [00:00<00:00, 7412.35samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:12,117 kur.model.executor:175]\u001b[0m Validation loss: 1.641\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:12,118 kur.model.executor:422]\u001b[0m Saving best historical validation weights: t3/cifar.best.valid.w\u001b[0m\n",
      "\n",
      "Epoch 3/20, loss=1.541: 100%|██████| 10000/10000 [00:02<00:00, 4452.24samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:14,370 kur.model.executor:390]\u001b[0m Training loss: 1.541\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:14,371 kur.model.executor:397]\u001b[0m Saving best historical training weights: t3/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.548: 100%|████████| 1600/1600 [00:00<00:00, 8700.24samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:14,562 kur.model.executor:175]\u001b[0m Validation loss: 1.548\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:14,562 kur.model.executor:422]\u001b[0m Saving best historical validation weights: t3/cifar.best.valid.w\u001b[0m\n",
      "\n",
      "Epoch 4/20, loss=1.444: 100%|██████| 10000/10000 [00:01<00:00, 5273.62samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:16,462 kur.model.executor:390]\u001b[0m Training loss: 1.444\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:16,462 kur.model.executor:397]\u001b[0m Saving best historical training weights: t3/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.510: 100%|████████| 1600/1600 [00:00<00:00, 8556.89samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:16,654 kur.model.executor:175]\u001b[0m Validation loss: 1.510\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:16,654 kur.model.executor:422]\u001b[0m Saving best historical validation weights: t3/cifar.best.valid.w\u001b[0m\n",
      "\n",
      "Epoch 5/20, loss=1.376: 100%|██████| 10000/10000 [00:02<00:00, 4149.76samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:19,075 kur.model.executor:390]\u001b[0m Training loss: 1.376\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:19,075 kur.model.executor:397]\u001b[0m Saving best historical training weights: t3/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.431: 100%|████████| 1600/1600 [00:00<00:00, 4356.86samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:19,448 kur.model.executor:175]\u001b[0m Validation loss: 1.431\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:19,448 kur.model.executor:422]\u001b[0m Saving best historical validation weights: t3/cifar.best.valid.w\u001b[0m\n",
      "\n",
      "Epoch 6/20, loss=1.319: 100%|██████| 10000/10000 [00:02<00:00, 3406.46samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:22,388 kur.model.executor:390]\u001b[0m Training loss: 1.319\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:22,388 kur.model.executor:397]\u001b[0m Saving best historical training weights: t3/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.411: 100%|████████| 1600/1600 [00:00<00:00, 4413.76samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:22,758 kur.model.executor:175]\u001b[0m Validation loss: 1.411\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:22,758 kur.model.executor:422]\u001b[0m Saving best historical validation weights: t3/cifar.best.valid.w\u001b[0m\n",
      "\n",
      "Epoch 7/20, loss=1.265: 100%|██████| 10000/10000 [00:02<00:00, 4796.83samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:24,846 kur.model.executor:390]\u001b[0m Training loss: 1.265\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:24,847 kur.model.executor:397]\u001b[0m Saving best historical training weights: t3/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.365: 100%|████████| 1600/1600 [00:00<00:00, 5337.14samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:25,151 kur.model.executor:175]\u001b[0m Validation loss: 1.365\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:25,151 kur.model.executor:422]\u001b[0m Saving best historical validation weights: t3/cifar.best.valid.w\u001b[0m\n",
      "\n",
      "Epoch 8/20, loss=1.226: 100%|██████| 10000/10000 [00:02<00:00, 4854.19samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:27,215 kur.model.executor:390]\u001b[0m Training loss: 1.226\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:27,215 kur.model.executor:397]\u001b[0m Saving best historical training weights: t3/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.401: 100%|████████| 1600/1600 [00:00<00:00, 6717.13samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:27,459 kur.model.executor:175]\u001b[0m Validation loss: 1.401\u001b[0m\n",
      "\n",
      "Epoch 9/20, loss=1.189: 100%|██████| 10000/10000 [00:02<00:00, 4833.21samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:29,530 kur.model.executor:390]\u001b[0m Training loss: 1.189\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:29,530 kur.model.executor:397]\u001b[0m Saving best historical training weights: t3/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.325: 100%|████████| 1600/1600 [00:00<00:00, 6882.16samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:29,768 kur.model.executor:175]\u001b[0m Validation loss: 1.325\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:29,769 kur.model.executor:422]\u001b[0m Saving best historical validation weights: t3/cifar.best.valid.w\u001b[0m\n",
      "\n",
      "Epoch 10/20, loss=1.155: 100%|█████| 10000/10000 [00:02<00:00, 4126.60samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:32,198 kur.model.executor:390]\u001b[0m Training loss: 1.155\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:32,198 kur.model.executor:397]\u001b[0m Saving best historical training weights: t3/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.275: 100%|████████| 1600/1600 [00:00<00:00, 6558.35samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:32,449 kur.model.executor:175]\u001b[0m Validation loss: 1.275\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:32,449 kur.model.executor:422]\u001b[0m Saving best historical validation weights: t3/cifar.best.valid.w\u001b[0m\n",
      "\n",
      "Epoch 11/20, loss=1.124: 100%|█████| 10000/10000 [00:02<00:00, 4612.79samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:34,624 kur.model.executor:390]\u001b[0m Training loss: 1.124\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:34,624 kur.model.executor:397]\u001b[0m Saving best historical training weights: t3/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.297: 100%|████████| 1600/1600 [00:00<00:00, 7161.01samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:34,852 kur.model.executor:175]\u001b[0m Validation loss: 1.297\u001b[0m\n",
      "\n",
      "Epoch 12/20, loss=1.093: 100%|█████| 10000/10000 [00:02<00:00, 4749.13samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:36,960 kur.model.executor:390]\u001b[0m Training loss: 1.093\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:36,961 kur.model.executor:397]\u001b[0m Saving best historical training weights: t3/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.298: 100%|████████| 1600/1600 [00:00<00:00, 6370.80samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:37,220 kur.model.executor:175]\u001b[0m Validation loss: 1.298\u001b[0m\n",
      "\n",
      "Epoch 13/20, loss=1.064: 100%|█████| 10000/10000 [00:02<00:00, 3999.67samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:39,723 kur.model.executor:390]\u001b[0m Training loss: 1.064\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:39,723 kur.model.executor:397]\u001b[0m Saving best historical training weights: t3/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.282: 100%|████████| 1600/1600 [00:00<00:00, 6927.25samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:39,959 kur.model.executor:175]\u001b[0m Validation loss: 1.282\u001b[0m\n",
      "\n",
      "Epoch 14/20, loss=1.049: 100%|█████| 10000/10000 [00:02<00:00, 4225.23samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:42,327 kur.model.executor:390]\u001b[0m Training loss: 1.049\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:42,328 kur.model.executor:397]\u001b[0m Saving best historical training weights: t3/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.285: 100%|████████| 1600/1600 [00:00<00:00, 4020.90samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:42,737 kur.model.executor:175]\u001b[0m Validation loss: 1.285\u001b[0m\n",
      "\n",
      "Epoch 15/20, loss=1.029: 100%|█████| 10000/10000 [00:03<00:00, 3258.72samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:45,808 kur.model.executor:390]\u001b[0m Training loss: 1.029\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:45,808 kur.model.executor:397]\u001b[0m Saving best historical training weights: t3/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.313: 100%|████████| 1600/1600 [00:00<00:00, 3002.92samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:46,346 kur.model.executor:175]\u001b[0m Validation loss: 1.313\u001b[0m\n",
      "\n",
      "Epoch 16/20, loss=1.000: 100%|█████| 10000/10000 [00:02<00:00, 3371.96samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:49,314 kur.model.executor:390]\u001b[0m Training loss: 1.000\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:49,314 kur.model.executor:397]\u001b[0m Saving best historical training weights: t3/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.244: 100%|████████| 1600/1600 [00:00<00:00, 4174.28samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:49,705 kur.model.executor:175]\u001b[0m Validation loss: 1.244\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:49,706 kur.model.executor:422]\u001b[0m Saving best historical validation weights: t3/cifar.best.valid.w\u001b[0m\n",
      "\n",
      "Epoch 17/20, loss=0.986: 100%|█████| 10000/10000 [00:02<00:00, 4469.39samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:51,949 kur.model.executor:390]\u001b[0m Training loss: 0.986\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:51,949 kur.model.executor:397]\u001b[0m Saving best historical training weights: t3/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.294: 100%|████████| 1600/1600 [00:00<00:00, 8014.51samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:52,154 kur.model.executor:175]\u001b[0m Validation loss: 1.294\u001b[0m\n",
      "\n",
      "Epoch 18/20, loss=0.968: 100%|█████| 10000/10000 [00:02<00:00, 4853.69samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:54,217 kur.model.executor:390]\u001b[0m Training loss: 0.968\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:54,217 kur.model.executor:397]\u001b[0m Saving best historical training weights: t3/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.352: 100%|████████| 1600/1600 [00:00<00:00, 7541.86samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:54,435 kur.model.executor:175]\u001b[0m Validation loss: 1.352\u001b[0m\n",
      "\n",
      "Epoch 19/20, loss=0.963: 100%|█████| 10000/10000 [00:02<00:00, 4310.08samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:56,757 kur.model.executor:390]\u001b[0m Training loss: 0.963\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:56,757 kur.model.executor:397]\u001b[0m Saving best historical training weights: t3/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.215: 100%|████████| 1600/1600 [00:00<00:00, 5476.13samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:57,060 kur.model.executor:175]\u001b[0m Validation loss: 1.215\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:57,060 kur.model.executor:422]\u001b[0m Saving best historical validation weights: t3/cifar.best.valid.w\u001b[0m\n",
      "\n",
      "Epoch 20/20, loss=0.930: 100%|█████| 10000/10000 [00:02<00:00, 4897.10samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:59,109 kur.model.executor:390]\u001b[0m Training loss: 0.930\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:59,109 kur.model.executor:397]\u001b[0m Saving best historical training weights: t3/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.259: 100%|████████| 1600/1600 [00:00<00:00, 8264.63samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:59,308 kur.model.executor:175]\u001b[0m Validation loss: 1.259\u001b[0m\n",
      "Completed 20 epochs.\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:59,309 kur.model.executor:210]\u001b[0m Saving most recent weights: t3/cifar.last.w\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!kur -v train dlnd_p2.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss_labels   validation_loss_labels\r\n",
      "training_loss_total    validation_loss_total\r\n"
     ]
    }
   ],
   "source": [
    "# %cd t3/\n",
    "!ls cifar-log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.0347259 ,  1.6929394 ,  1.54147506,  1.44420362,  1.3758949 ,\n",
       "        1.31863678,  1.26508343,  1.2262063 ,  1.18933189,  1.15463436,\n",
       "        1.12429321,  1.09335482,  1.06431651,  1.04932415,  1.02948546,\n",
       "        0.99968618,  0.98606402,  0.9677121 ,  0.96266526,  0.93041307], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kur.loggers import BinaryLogger\n",
    "training_loss = BinaryLogger.load_column('cifar-log', 'training_loss_total') \n",
    "validation_loss = BinaryLogger.load_column('cifar-log', 'validation_loss_total') \n",
    " \n",
    "training_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhBBCOEhSEEII4SBJQQghhMP/B5hdZSo3NNRtAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d1a3080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "epoch = list(range(1, 1+len(training_loss)))\n",
    "t_line, = plt.plot(epoch, training_loss, 'co-', label='Training Loss')\n",
    "v_line, = plt.plot(epoch, validation_loss, 'mo-', label='Validation Loss')\n",
    "plt.legend(handles=[t_line, v_line])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dlnd_p2_dropout.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile dlnd_p2_dropout.yml\n",
    "\n",
    "---\n",
    "settings:\n",
    "\n",
    "  # Where to get the data\n",
    "  cifar: &cifar\n",
    "    url: \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "    checksum: \"6d958be074577803d12ecdefd02955f39262c83c16fe9348329d7fe0b5c001ce\"\n",
    "    path: \"~/kur\"           # if kur does not have normalization or one-hot-encoding, ##################\n",
    "                            # without normalization and one-hot-encoding, the performance will be hurt, right? ####\n",
    "  # Backend to use                                      \n",
    "  backend:\n",
    "    name: keras\n",
    "\n",
    "  # Hyperparameters\n",
    "  cnn:\n",
    "    kernels: 20\n",
    "    size: [5, 5]\n",
    "    strides: [2, 2]                \n",
    "\n",
    "  pool:\n",
    "    size: [2,2]\n",
    "    strides: [2,2]\n",
    "    type: \"max\"                         # must use a string here, {max} won't work, doc didn't say it #############\n",
    "\n",
    "model:\n",
    "  - input: images                       # images are normalized from 0 to 1, labels are one-hot-encoding ##########\n",
    "  - convolution:                        # does kur do normalize and one-hot-encoding under the hood? ##############\n",
    "      kernels: \"{{cnn.kernels}}\"\n",
    "      size: \"{{cnn.size}}\"\n",
    "      strides: \"{{ cnn.strides }}\"\n",
    "  - activation: relu\n",
    "  - pool:\n",
    "      size: \"{{pool.size}}\"\n",
    "      strides: \"{{pool.strides}}\"\n",
    "      type: \"{{pool.type}}\"             \n",
    "  - flatten:\n",
    "  - dense: 15                           # p2 want a dropout applied here, but kur does not have yet?? ############\n",
    "  - dropout: 0.75\n",
    "  - dense: 10\n",
    "  - activation: softmax\n",
    "    name: labels\n",
    "\n",
    "train:\n",
    "  data:\n",
    "    - cifar:\n",
    "        <<: *cifar\n",
    "        parts: [1]                      # only use dataset part 1 to train\n",
    "  provider:\n",
    "    batch_size: 128\n",
    "    num_batches: 1000                   # the entire part 1 will be used\n",
    "  log: t3/cifar-log                   \n",
    "  epochs: 20\n",
    "  weights:\n",
    "    initial: t3/cifar.best.valid.w    \n",
    "    best: t3/cifar.best.train.w\n",
    "    last: t3/cifar.last.w\n",
    "\n",
    "  optimizer:\n",
    "    name: adam\n",
    "    learning_rate: 0.001\n",
    "\n",
    "validate:\n",
    "  data:\n",
    "    - cifar:\n",
    "       <<: *cifar\n",
    "       parts: 5                          # only use dataset part 5 as validation set\n",
    "  provider:\n",
    "    num_batches: 50                      # the project 2 only used 5000 data points as validation set\n",
    "  weights: t3/cifar.best.valid.w\n",
    "\n",
    "test: &test\n",
    "  data:\n",
    "    - cifar:\n",
    "       <<: *cifar\n",
    "       parts: test\n",
    "  weights: t3/cifar.best.valid.w\n",
    "  provider:\n",
    "    num_batches: 1000                     # the entire part test will be used\n",
    "\n",
    "evaluate:\n",
    "  <<: *test\n",
    "  destination: t3/cifar.results.pkl\n",
    "\n",
    "loss:\n",
    "  - target: labels                        # in the project: training loss and valid_accuracy are printed #############\n",
    "    name: categorical_crossentropy        # this should be a matter of personal taste, won't really affect anything##\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;37m[INFO 2017-02-28 22:28:36,640 kur.kurfile:638]\u001b[0m Parsing source: dlnd_p2_dropout.yml, included by top-level.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:28:36,658 kur.kurfile:79]\u001b[0m Parsing Kurfile...\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Natsume/miniconda2/envs/dlnd-tf-lab/bin/kur\", line 11, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/Users/Natsume/miniconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/kur/__main__.py\", line 269, in main\n",
      "    sys.exit(args.func(args) or 0)\n",
      "  File \"/Users/Natsume/miniconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/kur/__main__.py\", line 47, in train\n",
      "    spec = parse_kurfile(args.kurfile, args.engine)\n",
      "  File \"/Users/Natsume/miniconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/kur/__main__.py\", line 40, in parse_kurfile\n",
      "    spec.parse()\n",
      "  File \"/Users/Natsume/miniconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/kur/kurfile.py\", line 120, in parse\n",
      "    self.engine, builtin['model'], stack, required=True)\n",
      "  File \"/Users/Natsume/miniconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/kur/kurfile.py\", line 592, in _parse_model\n",
      "    for entry in self.data[key]\n",
      "  File \"/Users/Natsume/miniconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/kur/kurfile.py\", line 592, in <listcomp>\n",
      "    for entry in self.data[key]\n",
      "  File \"/Users/Natsume/miniconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/kur/containers/container.py\", line 71, in create_container_from_data\n",
      "    cls = Container.find_container_for_data(data)\n",
      "  File \"/Users/Natsume/miniconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/kur/containers/container.py\", line 82, in find_container_for_data\n",
      "    raise ValueError('No such container for data: {}'.format(data))\n",
      "ValueError: No such container for data: {'dropout': 0.75}\n"
     ]
    }
   ],
   "source": [
    "!kur -v train dlnd_p2_dropout.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
