{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dlnd_p2.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile dlnd_p2.yml\n",
    "\n",
    "---\n",
    "settings:\n",
    "\n",
    "  # Where to get the data\n",
    "  cifar: &cifar\n",
    "    url: \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "    checksum: \"6d958be074577803d12ecdefd02955f39262c83c16fe9348329d7fe0b5c001ce\"\n",
    "    path: \"~/kur\"           # if kur does not have normalization or one-hot-encoding, ##################\n",
    "                            # without normalization and one-hot-encoding, the performance will be hurt, right? ####\n",
    "  # Backend to use                                      \n",
    "  backend:\n",
    "    name: keras\n",
    "\n",
    "  # Hyperparameters\n",
    "  cnn:\n",
    "    kernels: 20\n",
    "    size: [5, 5]\n",
    "    strides: [2, 2]                \n",
    "\n",
    "  pool:\n",
    "    size: [2,2]\n",
    "    strides: [2,2]\n",
    "    type: \"max\"                         # must use a string here, {max} won't work, doc didn't say it #############\n",
    "\n",
    "model:\n",
    "  - input: images                       # images are normalized from 0 to 1, labels are one-hot-encoding ##########\n",
    "  - convolution:                        # does kur do normalize and one-hot-encoding under the hood? ##############\n",
    "      kernels: \"{{cnn.kernels}}\"\n",
    "      size: \"{{cnn.size}}\"\n",
    "      strides: \"{{ cnn.strides }}\"\n",
    "  - activation: relu\n",
    "  - pool:\n",
    "      size: \"{{pool.size}}\"\n",
    "      strides: \"{{pool.strides}}\"\n",
    "      type: \"{{pool.type}}\"             \n",
    "  - flatten:\n",
    "  - dense: 15                           # p2 want a dropout applied here, but kur does not have yet?? ############\n",
    "  - dense: 10\n",
    "  - activation: softmax\n",
    "    name: labels\n",
    "\n",
    "train:\n",
    "  data:\n",
    "    - cifar:\n",
    "        <<: *cifar\n",
    "        parts: [1]                      # only use dataset part 1 to train\n",
    "  provider:\n",
    "    batch_size: 128\n",
    "    num_batches: 1000                   # the entire part 1 will be used\n",
    "  log: t2/cifar-log                   \n",
    "  epochs: 20\n",
    "  weights:\n",
    "    initial: t2/cifar.best.valid.w    \n",
    "    best: t2/cifar.best.train.w\n",
    "    last: t2/cifar.last.w\n",
    "\n",
    "  optimizer:\n",
    "    name: adam\n",
    "    learning_rate: 0.001\n",
    "\n",
    "validate:\n",
    "  data:\n",
    "    - cifar:\n",
    "       <<: *cifar\n",
    "       parts: 5                          # only use dataset part 5 as validation set\n",
    "  provider:\n",
    "    num_batches: 50                      # the project 2 only used 5000 data points as validation set\n",
    "  weights: t2/cifar.best.valid.w\n",
    "\n",
    "test: &test\n",
    "  data:\n",
    "    - cifar:\n",
    "       <<: *cifar\n",
    "       parts: test\n",
    "  weights: t2/cifar.best.valid.w\n",
    "  provider:\n",
    "    num_batches: 1000                     # the entire part test will be used\n",
    "\n",
    "evaluate:\n",
    "  <<: *test\n",
    "  destination: t2/cifar.results.pkl\n",
    "\n",
    "loss:\n",
    "  - target: labels                        # in the project: training loss and valid_accuracy are printed #############\n",
    "    name: categorical_crossentropy        # this should be a matter of personal taste, won't really affect anything##\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/Natsume/Downloads/kur_road'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kur_Road.ipynb     dlnd_p2.yml        \u001b[34mt1\u001b[m\u001b[m/\r\n",
      "\u001b[34mcifar-log\u001b[m\u001b[m/         dlnd_p2_kur.ipynb  \u001b[34mt2\u001b[m\u001b[m/\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;37m[INFO 2017-02-28 21:20:36,347 kur.kurfile:638]\u001b[0m Parsing source: dlnd_p2.yml, included by top-level.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 21:20:36,361 kur.kurfile:79]\u001b[0m Parsing Kurfile...\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 21:20:36,393 kur.loggers.binary_logger:63]\u001b[0m Loading log data: t2/cifar-log\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 21:20:43,248 kur.providers.batch_provider:54]\u001b[0m Batch size set to: 128\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 21:20:43,248 kur.providers.batch_provider:60]\u001b[0m Maximum number of batches set to: 1000\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 21:20:49,334 kur.providers.batch_provider:54]\u001b[0m Batch size set to: 32\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 21:20:49,335 kur.providers.batch_provider:60]\u001b[0m Maximum number of batches set to: 50\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 21:20:49,335 kur.backend.backend:80]\u001b[0m Creating backend: keras\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 21:20:49,335 kur.backend.backend:83]\u001b[0m Backend variants: none\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 21:20:49,335 kur.backend.keras_backend:122]\u001b[0m No particular backend for Keras has been requested.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 21:20:50,349 kur.backend.keras_backend:191]\u001b[0m Keras is loaded. The backend is: theano\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 21:20:50,350 kur.model.model:260]\u001b[0m Enumerating the model containers.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 21:20:50,350 kur.model.model:265]\u001b[0m Assembling the model dependency graph.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 21:20:50,350 kur.model.model:280]\u001b[0m Connecting the model graph.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 21:20:51,380 kur.model.model:284]\u001b[0m Model inputs:  images\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 21:20:51,380 kur.model.model:285]\u001b[0m Model outputs: labels\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 21:20:52,635 kur.providers.batch_provider:54]\u001b[0m Batch size set to: 2\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 21:20:52,635 kur.providers.batch_provider:60]\u001b[0m Maximum number of batches set to: 1\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 21:20:52,638 kur.backend.keras_backend:654]\u001b[0m Waiting for model to finish compiling...\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 21:20:52,689 kur.model.executor:254]\u001b[0m Best historical training loss: 0.763\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 21:20:52,690 kur.model.executor:261]\u001b[0m Best historical validation loss: 1.275\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 21:20:52,690 kur.model.executor:272]\u001b[0m Restarting from epoch 41.\u001b[0m\n",
      "\n",
      "Epoch 41/60, loss=0.956: 100%|█████| 10000/10000 [00:02<00:00, 4587.65samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 21:20:54,881 kur.model.executor:390]\u001b[0m Training loss: 0.956\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 21:20:55,114 kur.providers.batch_provider:54]\u001b[0m Batch size set to: 2\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 21:20:55,114 kur.providers.batch_provider:60]\u001b[0m Maximum number of batches set to: 1\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 21:20:55,117 kur.backend.keras_backend:654]\u001b[0m Waiting for model to finish compiling...\u001b[0m\n",
      "Validating, loss=1.282: 100%|████████| 1600/1600 [00:00<00:00, 6450.44samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 21:20:55,367 kur.model.executor:175]\u001b[0m Validation loss: 1.282\u001b[0m\n",
      "\n",
      "Epoch 42/60, loss=0.937: 100%|█████| 10000/10000 [00:01<00:00, 5028.18samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 21:20:57,358 kur.model.executor:390]\u001b[0m Training loss: 0.937\u001b[0m\n",
      "Validating, loss=1.289: 100%|████████| 1600/1600 [00:00<00:00, 4881.83samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 21:20:57,688 kur.model.executor:175]\u001b[0m Validation loss: 1.289\u001b[0m\n",
      "\n",
      "Epoch 43/60, loss=0.918: 100%|█████| 10000/10000 [00:01<00:00, 5241.95samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 21:20:59,598 kur.model.executor:390]\u001b[0m Training loss: 0.918\u001b[0m\n",
      "Validating, loss=1.296: 100%|████████| 1600/1600 [00:00<00:00, 6590.38samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 21:20:59,842 kur.model.executor:175]\u001b[0m Validation loss: 1.296\u001b[0m\n",
      "\n",
      "Epoch 44/60, loss=0.910: 100%|█████| 10000/10000 [00:01<00:00, 5181.69samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 21:21:01,775 kur.model.executor:390]\u001b[0m Training loss: 0.910\u001b[0m\n",
      "Validating, loss=1.256: 100%|████████| 1600/1600 [00:00<00:00, 7261.57samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 21:21:01,998 kur.model.executor:175]\u001b[0m Validation loss: 1.256\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 21:21:01,998 kur.model.executor:422]\u001b[0m Saving best historical validation weights: t2/cifar.best.valid.w\u001b[0m\n",
      "\n",
      "Epoch 45/60, loss=0.893: 100%|█████| 10000/10000 [00:01<00:00, 5160.59samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 21:21:03,942 kur.model.executor:390]\u001b[0m Training loss: 0.893\u001b[0m\n",
      "Validating, loss=1.326: 100%|████████| 1600/1600 [00:00<00:00, 7067.83samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 21:21:04,170 kur.model.executor:175]\u001b[0m Validation loss: 1.326\u001b[0m\n",
      "\n",
      "Epoch 46/60, loss=0.881: 100%|█████| 10000/10000 [00:01<00:00, 5235.86samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 21:21:06,082 kur.model.executor:390]\u001b[0m Training loss: 0.881\u001b[0m\n",
      "Validating, loss=1.296: 100%|████████| 1600/1600 [00:00<00:00, 7392.32samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 21:21:06,301 kur.model.executor:175]\u001b[0m Validation loss: 1.296\u001b[0m\n",
      "\n",
      "Epoch 47/60, loss=0.868: 100%|█████| 10000/10000 [00:01<00:00, 5174.34samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 21:21:08,236 kur.model.executor:390]\u001b[0m Training loss: 0.868\u001b[0m\n",
      "Validating, loss=1.295: 100%|████████| 1600/1600 [00:00<00:00, 7581.71samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 21:21:08,449 kur.model.executor:175]\u001b[0m Validation loss: 1.295\u001b[0m\n",
      "\n",
      "Epoch 48/60, loss=0.850: 100%|█████| 10000/10000 [00:01<00:00, 5130.46samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 21:21:10,400 kur.model.executor:390]\u001b[0m Training loss: 0.850\u001b[0m\n",
      "Validating, loss=1.362: 100%|████████| 1600/1600 [00:00<00:00, 7391.40samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 21:21:10,619 kur.model.executor:175]\u001b[0m Validation loss: 1.362\u001b[0m\n",
      "\n",
      "Epoch 49/60, loss=0.842: 100%|█████| 10000/10000 [00:01<00:00, 5160.98samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 21:21:12,558 kur.model.executor:390]\u001b[0m Training loss: 0.842\u001b[0m\n",
      "Validating, loss=1.304: 100%|████████| 1600/1600 [00:00<00:00, 7793.62samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 21:21:12,765 kur.model.executor:175]\u001b[0m Validation loss: 1.304\u001b[0m\n",
      "\n",
      "Epoch 50/60, loss=0.830: 100%|█████| 10000/10000 [00:01<00:00, 5034.53samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 21:21:14,754 kur.model.executor:390]\u001b[0m Training loss: 0.830\u001b[0m\n",
      "Validating, loss=1.346: 100%|████████| 1600/1600 [00:00<00:00, 6823.55samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 21:21:14,990 kur.model.executor:175]\u001b[0m Validation loss: 1.346\u001b[0m\n",
      "\n",
      "Epoch 51/60, loss=0.830: 100%|█████| 10000/10000 [00:01<00:00, 5075.30samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 21:21:16,963 kur.model.executor:390]\u001b[0m Training loss: 0.830\u001b[0m\n",
      "Validating, loss=1.308: 100%|████████| 1600/1600 [00:00<00:00, 6943.87samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 21:21:17,195 kur.model.executor:175]\u001b[0m Validation loss: 1.308\u001b[0m\n",
      "\n",
      "Epoch 52/60, loss=0.810: 100%|█████| 10000/10000 [00:01<00:00, 5028.71samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 21:21:19,185 kur.model.executor:390]\u001b[0m Training loss: 0.810\u001b[0m\n",
      "Validating, loss=1.407: 100%|████████| 1600/1600 [00:00<00:00, 6797.22samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 21:21:19,423 kur.model.executor:175]\u001b[0m Validation loss: 1.407\u001b[0m\n",
      "\n",
      "Epoch 53/60, loss=0.806: 100%|█████| 10000/10000 [00:02<00:00, 4464.23samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 21:21:21,665 kur.model.executor:390]\u001b[0m Training loss: 0.806\u001b[0m\n",
      "Validating, loss=1.383: 100%|████████| 1600/1600 [00:00<00:00, 6912.85samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 21:21:21,899 kur.model.executor:175]\u001b[0m Validation loss: 1.383\u001b[0m\n",
      "\n",
      "Epoch 54/60, loss=0.799: 100%|█████| 10000/10000 [00:02<00:00, 4593.31samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 21:21:24,079 kur.model.executor:390]\u001b[0m Training loss: 0.799\u001b[0m\n",
      "Validating, loss=1.242: 100%|████████| 1600/1600 [00:00<00:00, 6869.23samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 21:21:24,314 kur.model.executor:175]\u001b[0m Validation loss: 1.242\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 21:21:24,314 kur.model.executor:422]\u001b[0m Saving best historical validation weights: t2/cifar.best.valid.w\u001b[0m\n",
      "\n",
      "Epoch 55/60, loss=0.780: 100%|█████| 10000/10000 [00:02<00:00, 4245.57samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 21:21:26,675 kur.model.executor:390]\u001b[0m Training loss: 0.780\u001b[0m\n",
      "Validating, loss=1.327: 100%|████████| 1600/1600 [00:00<00:00, 5666.95samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 21:21:26,961 kur.model.executor:175]\u001b[0m Validation loss: 1.327\u001b[0m\n",
      "\n",
      "Epoch 56/60, loss=0.776: 100%|█████| 10000/10000 [00:02<00:00, 4431.28samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 21:21:29,220 kur.model.executor:390]\u001b[0m Training loss: 0.776\u001b[0m\n",
      "Validating, loss=1.385: 100%|████████| 1600/1600 [00:00<00:00, 6463.00samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 21:21:29,470 kur.model.executor:175]\u001b[0m Validation loss: 1.385\u001b[0m\n",
      "\n",
      "Epoch 57/60, loss=0.764: 100%|█████| 10000/10000 [00:02<00:00, 4658.48samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 21:21:31,618 kur.model.executor:390]\u001b[0m Training loss: 0.764\u001b[0m\n",
      "Validating, loss=1.331: 100%|████████| 1600/1600 [00:00<00:00, 7292.35samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 21:21:31,840 kur.model.executor:175]\u001b[0m Validation loss: 1.331\u001b[0m\n",
      "\n",
      "Epoch 58/60, loss=0.745: 100%|█████| 10000/10000 [00:02<00:00, 4455.72samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 21:21:34,087 kur.model.executor:390]\u001b[0m Training loss: 0.745\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 21:21:34,087 kur.model.executor:397]\u001b[0m Saving best historical training weights: t2/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.309: 100%|████████| 1600/1600 [00:00<00:00, 5620.34samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 21:21:34,378 kur.model.executor:175]\u001b[0m Validation loss: 1.309\u001b[0m\n",
      "\n",
      "Epoch 59/60, loss=0.758: 100%|█████| 10000/10000 [00:02<00:00, 4939.66samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 21:21:36,405 kur.model.executor:390]\u001b[0m Training loss: 0.758\u001b[0m\n",
      "Validating, loss=1.488: 100%|████████| 1600/1600 [00:00<00:00, 5747.85samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 21:21:36,685 kur.model.executor:175]\u001b[0m Validation loss: 1.488\u001b[0m\n",
      "\n",
      "Epoch 60/60, loss=0.743: 100%|█████| 10000/10000 [00:02<00:00, 4012.81samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 21:21:39,179 kur.model.executor:390]\u001b[0m Training loss: 0.743\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 21:21:39,180 kur.model.executor:397]\u001b[0m Saving best historical training weights: t2/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.336: 100%|████████| 1600/1600 [00:00<00:00, 4569.64samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 21:21:39,546 kur.model.executor:175]\u001b[0m Validation loss: 1.336\u001b[0m\n",
      "Completed 60 epochs.\n",
      "\u001b[1;37m[INFO 2017-02-28 21:21:39,549 kur.model.executor:210]\u001b[0m Saving most recent weights: t2/cifar.last.w\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!kur -v train dlnd_p2.yml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
