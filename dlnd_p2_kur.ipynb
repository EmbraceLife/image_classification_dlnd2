{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewrite dlnd_project2 in kur\n",
    "- [original project_2 code](http://nbviewer.jupyter.org/github/EmbraceLife/image_classification_dlnd2/blob/master/cnn_cifar10.ipynb#annotations:5YK7tv1dEealzwMjXZx4WQ) in tensorflow\n",
    "- [detailed outline](https://github.com/EmbraceLife/image_classification_dlnd2/blob/master/outline.md) of the original code\n",
    "- kur code to replicate the original code is below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kur, by Deepgram -- deep learning made easy\r\n",
      "Version: 0.3.0\r\n",
      "Homepage: https://kur.deepgram.com\r\n"
     ]
    }
   ],
   "source": [
    "!kur --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dlnd_p2.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile dlnd_p2.yml\n",
    "\n",
    "---\n",
    "settings:\n",
    "\n",
    "  # Where to get the data\n",
    "  cifar: &cifar\n",
    "    url: \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "    checksum: \"6d958be074577803d12ecdefd02955f39262c83c16fe9348329d7fe0b5c001ce\"\n",
    "    path: \"~/kur\"           \n",
    "                        # I am tempting to read supplier source code to understand better how to use \n",
    "                        # each supplier (I see a few available, csv, pickle), do you think \n",
    "                        # it is a good idea to spend time on?  #################################################\n",
    "\n",
    "                        \n",
    "  # Backend to use                                      \n",
    "  backend:\n",
    "    name: keras         # how many backend does kur have, and what different do they make? ##################### \n",
    "                        # some complaint about keras performance, do you think it is an issue? #################\n",
    "\n",
    "  # Hyperparameters\n",
    "  cnn:\n",
    "    kernels: 20\n",
    "    size: [5, 5]\n",
    "    strides: [2, 2]                \n",
    "\n",
    "  pool:\n",
    "    size: [2,2]\n",
    "    strides: [2,2]\n",
    "    type: \"max\"                         \n",
    "\n",
    "model:\n",
    "  - input: images                       \n",
    "  - convolution:                        \n",
    "      kernels: \"{{cnn.kernels}}\"\n",
    "      size: \"{{cnn.size}}\"\n",
    "      strides: \"{{ cnn.strides }}\"\n",
    "  - activation: relu\n",
    "  - pool:\n",
    "      size: \"{{pool.size}}\"\n",
    "      strides: \"{{pool.strides}}\"\n",
    "      type: \"{{pool.type}}\"             \n",
    "  - flatten:\n",
    "  - dense: 15                           \n",
    "  - dense: 10\n",
    "  - activation: softmax\n",
    "    name: labels\n",
    "\n",
    "train:\n",
    "  data:\n",
    "    - cifar:\n",
    "        <<: *cifar\n",
    "        parts: [1]                      # only use dataset part 1 to train\n",
    "  provider:\n",
    "    batch_size: 128\n",
    "    num_batches: 1000                   # the entire part 1 will be used\n",
    "  log: t3/cifar-log                   \n",
    "  epochs: 20\n",
    "  weights:\n",
    "    initial: t3/cifar.best.valid.w    # do you think is it essential to implement weight visualization? #############\n",
    "    best: t3/cifar.best.train.w       # visualizing weights on different layers is used in explaining cnn on image###\n",
    "    last: t3/cifar.last.w             # or is it not important for help experimenting on models #####################\n",
    "\n",
    "  optimizer:\n",
    "    name: adam\n",
    "    learning_rate: 0.001\n",
    "\n",
    "validate:\n",
    "  data:\n",
    "    - cifar:\n",
    "       <<: *cifar\n",
    "       parts: 5                          # only use dataset part 5 as validation set\n",
    "  provider:\n",
    "    num_batches: 50                      # the project 2 only used 5000 data points as validation set\n",
    "  weights: t3/cifar.best.valid.w\n",
    "\n",
    "test: &test\n",
    "  data:\n",
    "    - cifar:\n",
    "       <<: *cifar\n",
    "       parts: test\n",
    "  weights: t3/cifar.best.valid.w\n",
    "  provider:\n",
    "    num_batches: 1000                     # the entire part test will be used\n",
    "\n",
    "evaluate:\n",
    "  <<: *test\n",
    "  destination: t3/cifar.results.pkl\n",
    "\n",
    "loss:\n",
    "  - target: labels                        # in the project: training loss and valid_accuracy are printed #############\n",
    "    name: categorical_crossentropy        # this should be a matter of personal taste, won't really affect anything##\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/Natsume/Downloads/kur_road'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kur_Road.ipynb       dlnd_p2_dropout.yml  \u001b[34mt1\u001b[m\u001b[m/\r\n",
      "\u001b[34mcifar-log\u001b[m\u001b[m/           dlnd_p2_kur.ipynb    \u001b[34mt2\u001b[m\u001b[m/\r\n",
      "dlnd_p2.yml          \u001b[34mkur\u001b[m\u001b[m/                 \u001b[34mt3\u001b[m\u001b[m/\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;37m[INFO 2017-02-28 22:28:48,972 kur.kurfile:638]\u001b[0m Parsing source: dlnd_p2.yml, included by top-level.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:28:48,989 kur.kurfile:79]\u001b[0m Parsing Kurfile...\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:28:49,038 kur.loggers.binary_logger:87]\u001b[0m Log does not exist. Creating path: t3/cifar-log\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:28:56,014 kur.providers.batch_provider:54]\u001b[0m Batch size set to: 128\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:28:56,014 kur.providers.batch_provider:60]\u001b[0m Maximum number of batches set to: 1000\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:02,413 kur.providers.batch_provider:54]\u001b[0m Batch size set to: 32\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:02,413 kur.providers.batch_provider:60]\u001b[0m Maximum number of batches set to: 50\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:02,414 kur.backend.backend:80]\u001b[0m Creating backend: keras\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:02,414 kur.backend.backend:83]\u001b[0m Backend variants: none\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:02,414 kur.backend.keras_backend:122]\u001b[0m No particular backend for Keras has been requested.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:03,578 kur.backend.keras_backend:191]\u001b[0m Keras is loaded. The backend is: theano\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:03,579 kur.model.model:260]\u001b[0m Enumerating the model containers.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:03,579 kur.model.model:265]\u001b[0m Assembling the model dependency graph.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:03,579 kur.model.model:280]\u001b[0m Connecting the model graph.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:04,579 kur.model.model:284]\u001b[0m Model inputs:  images\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:04,579 kur.model.model:285]\u001b[0m Model outputs: labels\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:04,580 kur.kurfile:310]\u001b[0m Ignoring missing initial weights: t3/cifar.best.valid.w. If this is undesireable, set \"must_exist\" to \"yes\" in the approriate \"weights\" section.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:05,910 kur.providers.batch_provider:54]\u001b[0m Batch size set to: 2\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:05,910 kur.providers.batch_provider:60]\u001b[0m Maximum number of batches set to: 1\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:05,913 kur.backend.keras_backend:654]\u001b[0m Waiting for model to finish compiling...\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:05,956 kur.model.executor:256]\u001b[0m No historical training loss available from logs.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:05,956 kur.model.executor:264]\u001b[0m No historical validation loss available from logs.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:05,956 kur.model.executor:270]\u001b[0m No previous epochs.\u001b[0m\n",
      "\n",
      "Epoch 1/20, loss=2.035: 100%|██████| 10000/10000 [00:02<00:00, 3344.74samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:08,954 kur.model.executor:390]\u001b[0m Training loss: 2.035\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:08,954 kur.model.executor:397]\u001b[0m Saving best historical training weights: t3/cifar.best.train.w\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:09,202 kur.providers.batch_provider:54]\u001b[0m Batch size set to: 2\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:09,203 kur.providers.batch_provider:60]\u001b[0m Maximum number of batches set to: 1\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:09,208 kur.backend.keras_backend:654]\u001b[0m Waiting for model to finish compiling...\u001b[0m\n",
      "Validating, loss=1.854: 100%|████████| 1600/1600 [00:00<00:00, 4341.85samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:09,585 kur.model.executor:175]\u001b[0m Validation loss: 1.854\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:09,585 kur.model.executor:422]\u001b[0m Saving best historical validation weights: t3/cifar.best.valid.w\u001b[0m\n",
      "\n",
      "Epoch 2/20, loss=1.693: 100%|██████| 10000/10000 [00:02<00:00, 4339.70samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:11,897 kur.model.executor:390]\u001b[0m Training loss: 1.693\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:11,897 kur.model.executor:397]\u001b[0m Saving best historical training weights: t3/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.641: 100%|████████| 1600/1600 [00:00<00:00, 7412.35samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:12,117 kur.model.executor:175]\u001b[0m Validation loss: 1.641\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:12,118 kur.model.executor:422]\u001b[0m Saving best historical validation weights: t3/cifar.best.valid.w\u001b[0m\n",
      "\n",
      "Epoch 3/20, loss=1.541: 100%|██████| 10000/10000 [00:02<00:00, 4452.24samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:14,370 kur.model.executor:390]\u001b[0m Training loss: 1.541\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:14,371 kur.model.executor:397]\u001b[0m Saving best historical training weights: t3/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.548: 100%|████████| 1600/1600 [00:00<00:00, 8700.24samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:14,562 kur.model.executor:175]\u001b[0m Validation loss: 1.548\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:14,562 kur.model.executor:422]\u001b[0m Saving best historical validation weights: t3/cifar.best.valid.w\u001b[0m\n",
      "\n",
      "Epoch 4/20, loss=1.444: 100%|██████| 10000/10000 [00:01<00:00, 5273.62samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:16,462 kur.model.executor:390]\u001b[0m Training loss: 1.444\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:16,462 kur.model.executor:397]\u001b[0m Saving best historical training weights: t3/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.510: 100%|████████| 1600/1600 [00:00<00:00, 8556.89samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:16,654 kur.model.executor:175]\u001b[0m Validation loss: 1.510\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:16,654 kur.model.executor:422]\u001b[0m Saving best historical validation weights: t3/cifar.best.valid.w\u001b[0m\n",
      "\n",
      "Epoch 5/20, loss=1.376: 100%|██████| 10000/10000 [00:02<00:00, 4149.76samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:19,075 kur.model.executor:390]\u001b[0m Training loss: 1.376\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:19,075 kur.model.executor:397]\u001b[0m Saving best historical training weights: t3/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.431: 100%|████████| 1600/1600 [00:00<00:00, 4356.86samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:19,448 kur.model.executor:175]\u001b[0m Validation loss: 1.431\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:19,448 kur.model.executor:422]\u001b[0m Saving best historical validation weights: t3/cifar.best.valid.w\u001b[0m\n",
      "\n",
      "Epoch 6/20, loss=1.319: 100%|██████| 10000/10000 [00:02<00:00, 3406.46samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:22,388 kur.model.executor:390]\u001b[0m Training loss: 1.319\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:22,388 kur.model.executor:397]\u001b[0m Saving best historical training weights: t3/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.411: 100%|████████| 1600/1600 [00:00<00:00, 4413.76samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:22,758 kur.model.executor:175]\u001b[0m Validation loss: 1.411\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:22,758 kur.model.executor:422]\u001b[0m Saving best historical validation weights: t3/cifar.best.valid.w\u001b[0m\n",
      "\n",
      "Epoch 7/20, loss=1.265: 100%|██████| 10000/10000 [00:02<00:00, 4796.83samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:24,846 kur.model.executor:390]\u001b[0m Training loss: 1.265\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:24,847 kur.model.executor:397]\u001b[0m Saving best historical training weights: t3/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.365: 100%|████████| 1600/1600 [00:00<00:00, 5337.14samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:25,151 kur.model.executor:175]\u001b[0m Validation loss: 1.365\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:25,151 kur.model.executor:422]\u001b[0m Saving best historical validation weights: t3/cifar.best.valid.w\u001b[0m\n",
      "\n",
      "Epoch 8/20, loss=1.226: 100%|██████| 10000/10000 [00:02<00:00, 4854.19samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:27,215 kur.model.executor:390]\u001b[0m Training loss: 1.226\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:27,215 kur.model.executor:397]\u001b[0m Saving best historical training weights: t3/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.401: 100%|████████| 1600/1600 [00:00<00:00, 6717.13samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:27,459 kur.model.executor:175]\u001b[0m Validation loss: 1.401\u001b[0m\n",
      "\n",
      "Epoch 9/20, loss=1.189: 100%|██████| 10000/10000 [00:02<00:00, 4833.21samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:29,530 kur.model.executor:390]\u001b[0m Training loss: 1.189\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:29,530 kur.model.executor:397]\u001b[0m Saving best historical training weights: t3/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.325: 100%|████████| 1600/1600 [00:00<00:00, 6882.16samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:29,768 kur.model.executor:175]\u001b[0m Validation loss: 1.325\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:29,769 kur.model.executor:422]\u001b[0m Saving best historical validation weights: t3/cifar.best.valid.w\u001b[0m\n",
      "\n",
      "Epoch 10/20, loss=1.155: 100%|█████| 10000/10000 [00:02<00:00, 4126.60samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:32,198 kur.model.executor:390]\u001b[0m Training loss: 1.155\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:32,198 kur.model.executor:397]\u001b[0m Saving best historical training weights: t3/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.275: 100%|████████| 1600/1600 [00:00<00:00, 6558.35samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:32,449 kur.model.executor:175]\u001b[0m Validation loss: 1.275\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:32,449 kur.model.executor:422]\u001b[0m Saving best historical validation weights: t3/cifar.best.valid.w\u001b[0m\n",
      "\n",
      "Epoch 11/20, loss=1.124: 100%|█████| 10000/10000 [00:02<00:00, 4612.79samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:34,624 kur.model.executor:390]\u001b[0m Training loss: 1.124\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:34,624 kur.model.executor:397]\u001b[0m Saving best historical training weights: t3/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.297: 100%|████████| 1600/1600 [00:00<00:00, 7161.01samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:34,852 kur.model.executor:175]\u001b[0m Validation loss: 1.297\u001b[0m\n",
      "\n",
      "Epoch 12/20, loss=1.093: 100%|█████| 10000/10000 [00:02<00:00, 4749.13samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:36,960 kur.model.executor:390]\u001b[0m Training loss: 1.093\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:36,961 kur.model.executor:397]\u001b[0m Saving best historical training weights: t3/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.298: 100%|████████| 1600/1600 [00:00<00:00, 6370.80samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:37,220 kur.model.executor:175]\u001b[0m Validation loss: 1.298\u001b[0m\n",
      "\n",
      "Epoch 13/20, loss=1.064: 100%|█████| 10000/10000 [00:02<00:00, 3999.67samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:39,723 kur.model.executor:390]\u001b[0m Training loss: 1.064\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:39,723 kur.model.executor:397]\u001b[0m Saving best historical training weights: t3/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.282: 100%|████████| 1600/1600 [00:00<00:00, 6927.25samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:39,959 kur.model.executor:175]\u001b[0m Validation loss: 1.282\u001b[0m\n",
      "\n",
      "Epoch 14/20, loss=1.049: 100%|█████| 10000/10000 [00:02<00:00, 4225.23samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:42,327 kur.model.executor:390]\u001b[0m Training loss: 1.049\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:42,328 kur.model.executor:397]\u001b[0m Saving best historical training weights: t3/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.285: 100%|████████| 1600/1600 [00:00<00:00, 4020.90samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:42,737 kur.model.executor:175]\u001b[0m Validation loss: 1.285\u001b[0m\n",
      "\n",
      "Epoch 15/20, loss=1.029: 100%|█████| 10000/10000 [00:03<00:00, 3258.72samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:45,808 kur.model.executor:390]\u001b[0m Training loss: 1.029\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:45,808 kur.model.executor:397]\u001b[0m Saving best historical training weights: t3/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.313: 100%|████████| 1600/1600 [00:00<00:00, 3002.92samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:46,346 kur.model.executor:175]\u001b[0m Validation loss: 1.313\u001b[0m\n",
      "\n",
      "Epoch 16/20, loss=1.000: 100%|█████| 10000/10000 [00:02<00:00, 3371.96samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:49,314 kur.model.executor:390]\u001b[0m Training loss: 1.000\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:49,314 kur.model.executor:397]\u001b[0m Saving best historical training weights: t3/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.244: 100%|████████| 1600/1600 [00:00<00:00, 4174.28samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:49,705 kur.model.executor:175]\u001b[0m Validation loss: 1.244\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:49,706 kur.model.executor:422]\u001b[0m Saving best historical validation weights: t3/cifar.best.valid.w\u001b[0m\n",
      "\n",
      "Epoch 17/20, loss=0.986: 100%|█████| 10000/10000 [00:02<00:00, 4469.39samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:51,949 kur.model.executor:390]\u001b[0m Training loss: 0.986\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:51,949 kur.model.executor:397]\u001b[0m Saving best historical training weights: t3/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.294: 100%|████████| 1600/1600 [00:00<00:00, 8014.51samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:52,154 kur.model.executor:175]\u001b[0m Validation loss: 1.294\u001b[0m\n",
      "\n",
      "Epoch 18/20, loss=0.968: 100%|█████| 10000/10000 [00:02<00:00, 4853.69samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:54,217 kur.model.executor:390]\u001b[0m Training loss: 0.968\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:54,217 kur.model.executor:397]\u001b[0m Saving best historical training weights: t3/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.352: 100%|████████| 1600/1600 [00:00<00:00, 7541.86samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:54,435 kur.model.executor:175]\u001b[0m Validation loss: 1.352\u001b[0m\n",
      "\n",
      "Epoch 19/20, loss=0.963: 100%|█████| 10000/10000 [00:02<00:00, 4310.08samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:56,757 kur.model.executor:390]\u001b[0m Training loss: 0.963\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:56,757 kur.model.executor:397]\u001b[0m Saving best historical training weights: t3/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.215: 100%|████████| 1600/1600 [00:00<00:00, 5476.13samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:57,060 kur.model.executor:175]\u001b[0m Validation loss: 1.215\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:57,060 kur.model.executor:422]\u001b[0m Saving best historical validation weights: t3/cifar.best.valid.w\u001b[0m\n",
      "\n",
      "Epoch 20/20, loss=0.930: 100%|█████| 10000/10000 [00:02<00:00, 4897.10samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:59,109 kur.model.executor:390]\u001b[0m Training loss: 0.930\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:59,109 kur.model.executor:397]\u001b[0m Saving best historical training weights: t3/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.259: 100%|████████| 1600/1600 [00:00<00:00, 8264.63samples/s]\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:59,308 kur.model.executor:175]\u001b[0m Validation loss: 1.259\u001b[0m\n",
      "Completed 20 epochs.\n",
      "\u001b[1;37m[INFO 2017-02-28 22:29:59,309 kur.model.executor:210]\u001b[0m Saving most recent weights: t3/cifar.last.w\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!kur -v train dlnd_p2.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss_labels   validation_loss_labels\r\n",
      "training_loss_total    validation_loss_total\r\n"
     ]
    }
   ],
   "source": [
    "# %cd t3/\n",
    "!ls cifar-log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.0347259 ,  1.6929394 ,  1.54147506,  1.44420362,  1.3758949 ,\n",
       "        1.31863678,  1.26508343,  1.2262063 ,  1.18933189,  1.15463436,\n",
       "        1.12429321,  1.09335482,  1.06431651,  1.04932415,  1.02948546,\n",
       "        0.99968618,  0.98606402,  0.9677121 ,  0.96266526,  0.93041307], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kur.loggers import BinaryLogger\n",
    "training_loss = BinaryLogger.load_column('cifar-log', 'training_loss_total') \n",
    "validation_loss = BinaryLogger.load_column('cifar-log', 'validation_loss_total') \n",
    " \n",
    "training_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhBBCOEhSEEII4SBJQQghhMP/B5hdZSo3NNRtAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d1a3080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "epoch = list(range(1, 1+len(training_loss)))\n",
    "t_line, = plt.plot(epoch, training_loss, 'co-', label='Training Loss')\n",
    "v_line, = plt.plot(epoch, validation_loss, 'mo-', label='Validation Loss')\n",
    "plt.legend(handles=[t_line, v_line])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dlnd_p2_dropout.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile dlnd_p2_dropout.yml\n",
    "\n",
    "---\n",
    "settings:\n",
    "\n",
    "  # Where to get the data\n",
    "  cifar: &cifar\n",
    "    url: \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "    checksum: \"6d958be074577803d12ecdefd02955f39262c83c16fe9348329d7fe0b5c001ce\"\n",
    "    path: \"~/kur\"           # if kur does not have normalization or one-hot-encoding, ##################\n",
    "                            # without normalization and one-hot-encoding, the performance will be hurt, right? ####\n",
    "  # Backend to use                                      \n",
    "  backend:\n",
    "    name: keras\n",
    "\n",
    "  # Hyperparameters\n",
    "  cnn:\n",
    "    kernels: 20\n",
    "    size: [5, 5]\n",
    "    strides: [2, 2]                \n",
    "\n",
    "  pool:\n",
    "    size: [2,2]\n",
    "    strides: [2,2]\n",
    "    type: max                         # must use a string here, {max} won't work, doc didn't say it #############\n",
    "\n",
    "model:\n",
    "  - input: images                       # images are normalized from 0 to 1, labels are one-hot-encoding ##########\n",
    "  - convolution:                        # does kur do normalize and one-hot-encoding under the hood? ##############\n",
    "      kernels: \"{{cnn.kernels}}\"\n",
    "      size: \"{{cnn.size}}\"\n",
    "      strides: \"{{ cnn.strides }}\"\n",
    "  - activation: relu\n",
    "  - pool:\n",
    "      size: \"{{pool.size}}\"\n",
    "      strides: \"{{pool.strides}}\"\n",
    "      type: \"{{pool.type}}\"             \n",
    "  - flatten:\n",
    "  - dense: 15                           # p2 want a dropout applied here, but kur does not have yet?? ############\n",
    "  - dropout: 0.25\n",
    "  - dense: 10\n",
    "  - activation: softmax\n",
    "    name: labels\n",
    "\n",
    "train:\n",
    "  data:\n",
    "    - cifar:\n",
    "        <<: *cifar\n",
    "        parts: [1]                      # only use dataset part 1 to train\n",
    "  provider:\n",
    "    batch_size: 128\n",
    "    num_batches: 1000                   # the entire part 1 will be used\n",
    "  log: t1_dp0.25/cifar-log                   \n",
    "  epochs: 20\n",
    "  weights:\n",
    "    initial: t1_dp0.25/cifar.best.valid.w    \n",
    "    best: t1_dp0.25/cifar.best.train.w\n",
    "    last: t1_dp0.25/cifar.last.w\n",
    "\n",
    "  optimizer:\n",
    "    name: adam\n",
    "    learning_rate: 0.001\n",
    "\n",
    "validate:\n",
    "  data:\n",
    "    - cifar:\n",
    "       <<: *cifar\n",
    "       parts: 5                          # only use dataset part 5 as validation set\n",
    "  provider:\n",
    "    batch_size: 128\n",
    "    num_batches: 50                      # the project 2 only used 5000 data points as validation set\n",
    "  weights: t1_dp0.25/cifar.best.valid.w\n",
    "\n",
    "test: &test\n",
    "  data:\n",
    "    - cifar:\n",
    "       <<: *cifar\n",
    "       parts: test\n",
    "  weights: t1_dp0.25/cifar.best.valid.w\n",
    "  provider:\n",
    "    batch_size: 128\n",
    "    num_batches: 1000                     # the entire part test will be used\n",
    "\n",
    "evaluate:\n",
    "  <<: *test\n",
    "  destination: t1_dp0.25/cifar.results.pkl\n",
    "\n",
    "loss:\n",
    "  - target: labels                        # in the project: training loss and valid_accuracy are printed #############\n",
    "    name: categorical_crossentropy        # this should be a matter of personal taste, won't really affect anything##\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;37m[INFO 2017-03-01 07:34:45,517 kur.kurfile:699]\u001b[0m Parsing source: dlnd_p2_dropout.yml, included by top-level.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:34:45,533 kur.kurfile:82]\u001b[0m Parsing Kurfile...\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:34:45,558 kur.loggers.binary_logger:107]\u001b[0m Log does not exist. Creating path: t1_dp0.25/cifar-log\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:34:58,394 kur.backend.backend:80]\u001b[0m Creating backend: keras\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:34:58,394 kur.backend.backend:83]\u001b[0m Backend variants: none\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:34:58,394 kur.backend.keras_backend:122]\u001b[0m No particular backend for Keras has been requested.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:34:59,341 kur.backend.keras_backend:195]\u001b[0m Keras is loaded. The backend is: theano\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:34:59,341 kur.model.model:260]\u001b[0m Enumerating the model containers.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:34:59,342 kur.model.model:265]\u001b[0m Assembling the model dependency graph.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:34:59,342 kur.model.model:280]\u001b[0m Connecting the model graph.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:00,404 kur.model.model:284]\u001b[0m Model inputs:  images\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:00,404 kur.model.model:285]\u001b[0m Model outputs: labels\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:00,404 kur.kurfile:357]\u001b[0m Ignoring missing initial weights: t1_dp0.25/cifar.best.valid.w. If this is undesireable, set \"must_exist\" to \"yes\" in the approriate \"weights\" section.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:00,404 kur.model.executor:315]\u001b[0m No historical training loss available from logs.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:00,404 kur.model.executor:323]\u001b[0m No historical validation loss available from logs.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:00,404 kur.model.executor:329]\u001b[0m No previous epochs.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:01,829 kur.backend.keras_backend:666]\u001b[0m Waiting for model to finish compiling...\u001b[0m\n",
      "\n",
      "Epoch 1/20, loss=2.080: 100%|██████| 10000/10000 [00:02<00:00, 3728.18samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:04,562 kur.model.executor:464]\u001b[0m Training loss: 2.080\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:04,562 kur.model.executor:471]\u001b[0m Saving best historical training weights: t1_dp0.25/cifar.best.train.w\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:04,953 kur.backend.keras_backend:666]\u001b[0m Waiting for model to finish compiling...\u001b[0m\n",
      "Validating, loss=1.874: 100%|███████| 6400/6400 [00:00<00:00, 10436.54samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:05,569 kur.model.executor:197]\u001b[0m Validation loss: 1.874\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:05,570 kur.model.executor:413]\u001b[0m Saving best historical validation weights: t1_dp0.25/cifar.best.valid.w\u001b[0m\n",
      "\n",
      "Epoch 2/20, loss=1.801: 100%|██████| 10000/10000 [00:02<00:00, 4985.77samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:07,580 kur.model.executor:464]\u001b[0m Training loss: 1.801\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:07,580 kur.model.executor:471]\u001b[0m Saving best historical training weights: t1_dp0.25/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.672: 100%|███████| 6400/6400 [00:00<00:00, 10588.70samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:08,196 kur.model.executor:197]\u001b[0m Validation loss: 1.672\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:08,196 kur.model.executor:413]\u001b[0m Saving best historical validation weights: t1_dp0.25/cifar.best.valid.w\u001b[0m\n",
      "\n",
      "Epoch 3/20, loss=1.646: 100%|██████| 10000/10000 [00:02<00:00, 4988.95samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:10,206 kur.model.executor:464]\u001b[0m Training loss: 1.646\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:10,206 kur.model.executor:471]\u001b[0m Saving best historical training weights: t1_dp0.25/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.578: 100%|███████| 6400/6400 [00:00<00:00, 11537.31samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:10,768 kur.model.executor:197]\u001b[0m Validation loss: 1.578\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:10,768 kur.model.executor:413]\u001b[0m Saving best historical validation weights: t1_dp0.25/cifar.best.valid.w\u001b[0m\n",
      "\n",
      "Epoch 4/20, loss=1.537: 100%|██████| 10000/10000 [00:02<00:00, 4735.37samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:12,886 kur.model.executor:464]\u001b[0m Training loss: 1.537\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:12,887 kur.model.executor:471]\u001b[0m Saving best historical training weights: t1_dp0.25/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.490: 100%|███████| 6400/6400 [00:00<00:00, 10859.18samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:13,486 kur.model.executor:197]\u001b[0m Validation loss: 1.490\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:13,486 kur.model.executor:413]\u001b[0m Saving best historical validation weights: t1_dp0.25/cifar.best.valid.w\u001b[0m\n",
      "\n",
      "Epoch 5/20, loss=1.470: 100%|██████| 10000/10000 [00:02<00:00, 4768.44samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:15,588 kur.model.executor:464]\u001b[0m Training loss: 1.470\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:15,588 kur.model.executor:471]\u001b[0m Saving best historical training weights: t1_dp0.25/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.411: 100%|███████| 6400/6400 [00:00<00:00, 10774.77samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:16,189 kur.model.executor:197]\u001b[0m Validation loss: 1.411\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:16,190 kur.model.executor:413]\u001b[0m Saving best historical validation weights: t1_dp0.25/cifar.best.valid.w\u001b[0m\n",
      "\n",
      "Epoch 6/20, loss=1.406: 100%|██████| 10000/10000 [00:02<00:00, 4293.21samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:18,523 kur.model.executor:464]\u001b[0m Training loss: 1.406\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:18,524 kur.model.executor:471]\u001b[0m Saving best historical training weights: t1_dp0.25/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.384: 100%|███████| 6400/6400 [00:00<00:00, 10308.95samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:19,153 kur.model.executor:197]\u001b[0m Validation loss: 1.384\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:19,154 kur.model.executor:413]\u001b[0m Saving best historical validation weights: t1_dp0.25/cifar.best.valid.w\u001b[0m\n",
      "\n",
      "Epoch 7/20, loss=1.371: 100%|██████| 10000/10000 [00:02<00:00, 4549.75samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:21,358 kur.model.executor:464]\u001b[0m Training loss: 1.371\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:21,358 kur.model.executor:471]\u001b[0m Saving best historical training weights: t1_dp0.25/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.368: 100%|████████| 6400/6400 [00:00<00:00, 4923.30samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:21,993 kur.model.executor:197]\u001b[0m Validation loss: 1.368\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:21,994 kur.model.executor:413]\u001b[0m Saving best historical validation weights: t1_dp0.25/cifar.best.valid.w\u001b[0m\n",
      "\n",
      "Epoch 8/20, loss=1.330: 100%|██████| 10000/10000 [00:02<00:00, 4738.82samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:24,111 kur.model.executor:464]\u001b[0m Training loss: 1.330\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:24,111 kur.model.executor:471]\u001b[0m Saving best historical training weights: t1_dp0.25/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.328: 100%|███████| 6400/6400 [00:00<00:00, 10351.03samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:24,737 kur.model.executor:197]\u001b[0m Validation loss: 1.328\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:24,738 kur.model.executor:413]\u001b[0m Saving best historical validation weights: t1_dp0.25/cifar.best.valid.w\u001b[0m\n",
      "\n",
      "Epoch 9/20, loss=1.303: 100%|██████| 10000/10000 [00:02<00:00, 4794.48samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:26,830 kur.model.executor:464]\u001b[0m Training loss: 1.303\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:26,830 kur.model.executor:471]\u001b[0m Saving best historical training weights: t1_dp0.25/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.339: 100%|███████| 6400/6400 [00:00<00:00, 11131.73samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:27,412 kur.model.executor:197]\u001b[0m Validation loss: 1.339\u001b[0m\n",
      "\n",
      "Epoch 10/20, loss=1.294: 100%|█████| 10000/10000 [00:02<00:00, 4693.60samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:29,546 kur.model.executor:464]\u001b[0m Training loss: 1.294\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:29,546 kur.model.executor:471]\u001b[0m Saving best historical training weights: t1_dp0.25/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.310: 100%|███████| 6400/6400 [00:00<00:00, 11321.00samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:30,118 kur.model.executor:197]\u001b[0m Validation loss: 1.310\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:30,119 kur.model.executor:413]\u001b[0m Saving best historical validation weights: t1_dp0.25/cifar.best.valid.w\u001b[0m\n",
      "\n",
      "Epoch 11/20, loss=1.253: 100%|█████| 10000/10000 [00:02<00:00, 4719.59samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:32,243 kur.model.executor:464]\u001b[0m Training loss: 1.253\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:32,243 kur.model.executor:471]\u001b[0m Saving best historical training weights: t1_dp0.25/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.293: 100%|███████| 6400/6400 [00:00<00:00, 10417.43samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:32,866 kur.model.executor:197]\u001b[0m Validation loss: 1.293\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:32,867 kur.model.executor:413]\u001b[0m Saving best historical validation weights: t1_dp0.25/cifar.best.valid.w\u001b[0m\n",
      "\n",
      "Epoch 12/20, loss=1.233: 100%|█████| 10000/10000 [00:02<00:00, 4647.94samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:35,024 kur.model.executor:464]\u001b[0m Training loss: 1.233\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:35,024 kur.model.executor:471]\u001b[0m Saving best historical training weights: t1_dp0.25/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.268: 100%|████████| 6400/6400 [00:00<00:00, 9125.34samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:35,748 kur.model.executor:197]\u001b[0m Validation loss: 1.268\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:35,748 kur.model.executor:413]\u001b[0m Saving best historical validation weights: t1_dp0.25/cifar.best.valid.w\u001b[0m\n",
      "\n",
      "Epoch 13/20, loss=1.213: 100%|█████| 10000/10000 [00:02<00:00, 4052.58samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:38,221 kur.model.executor:464]\u001b[0m Training loss: 1.213\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:38,221 kur.model.executor:471]\u001b[0m Saving best historical training weights: t1_dp0.25/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.263: 100%|███████| 6400/6400 [00:00<00:00, 10334.80samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:38,848 kur.model.executor:197]\u001b[0m Validation loss: 1.263\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:38,848 kur.model.executor:413]\u001b[0m Saving best historical validation weights: t1_dp0.25/cifar.best.valid.w\u001b[0m\n",
      "\n",
      "Epoch 14/20, loss=1.192: 100%|█████| 10000/10000 [00:02<00:00, 4059.79samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:41,316 kur.model.executor:464]\u001b[0m Training loss: 1.192\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:41,316 kur.model.executor:471]\u001b[0m Saving best historical training weights: t1_dp0.25/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.267: 100%|███████| 6400/6400 [00:00<00:00, 10624.42samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:41,927 kur.model.executor:197]\u001b[0m Validation loss: 1.267\u001b[0m\n",
      "\n",
      "Epoch 15/20, loss=1.179: 100%|█████| 10000/10000 [00:03<00:00, 3174.28samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:45,081 kur.model.executor:464]\u001b[0m Training loss: 1.179\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:45,081 kur.model.executor:471]\u001b[0m Saving best historical training weights: t1_dp0.25/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.250: 100%|███████| 6400/6400 [00:00<00:00, 10746.89samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:45,686 kur.model.executor:197]\u001b[0m Validation loss: 1.250\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:45,686 kur.model.executor:413]\u001b[0m Saving best historical validation weights: t1_dp0.25/cifar.best.valid.w\u001b[0m\n",
      "\n",
      "Epoch 16/20, loss=1.155: 100%|█████| 10000/10000 [00:02<00:00, 3838.08samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:48,298 kur.model.executor:464]\u001b[0m Training loss: 1.155\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:48,298 kur.model.executor:471]\u001b[0m Saving best historical training weights: t1_dp0.25/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.249: 100%|███████| 6400/6400 [00:00<00:00, 10454.68samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:48,938 kur.model.executor:197]\u001b[0m Validation loss: 1.249\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:48,939 kur.model.executor:413]\u001b[0m Saving best historical validation weights: t1_dp0.25/cifar.best.valid.w\u001b[0m\n",
      "\n",
      "Epoch 17/20, loss=1.141: 100%|█████| 10000/10000 [00:02<00:00, 3516.54samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:51,791 kur.model.executor:464]\u001b[0m Training loss: 1.141\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:51,791 kur.model.executor:471]\u001b[0m Saving best historical training weights: t1_dp0.25/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.235: 100%|███████| 6400/6400 [00:00<00:00, 10439.16samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:52,412 kur.model.executor:197]\u001b[0m Validation loss: 1.235\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:52,412 kur.model.executor:413]\u001b[0m Saving best historical validation weights: t1_dp0.25/cifar.best.valid.w\u001b[0m\n",
      "\n",
      "Epoch 18/20, loss=1.123: 100%|█████| 10000/10000 [00:03<00:00, 3276.45samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:55,469 kur.model.executor:464]\u001b[0m Training loss: 1.123\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:55,470 kur.model.executor:471]\u001b[0m Saving best historical training weights: t1_dp0.25/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.238: 100%|████████| 6400/6400 [00:00<00:00, 7017.06samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:56,390 kur.model.executor:197]\u001b[0m Validation loss: 1.238\u001b[0m\n",
      "\n",
      "Epoch 19/20, loss=1.121: 100%|█████| 10000/10000 [00:02<00:00, 4330.83samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:58,703 kur.model.executor:464]\u001b[0m Training loss: 1.121\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:58,704 kur.model.executor:471]\u001b[0m Saving best historical training weights: t1_dp0.25/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.224: 100%|███████| 6400/6400 [00:00<00:00, 11403.63samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:59,272 kur.model.executor:197]\u001b[0m Validation loss: 1.224\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:35:59,272 kur.model.executor:413]\u001b[0m Saving best historical validation weights: t1_dp0.25/cifar.best.valid.w\u001b[0m\n",
      "\n",
      "Epoch 20/20, loss=1.107: 100%|█████| 10000/10000 [00:02<00:00, 4730.36samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-01 07:36:01,393 kur.model.executor:464]\u001b[0m Training loss: 1.107\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 07:36:01,394 kur.model.executor:471]\u001b[0m Saving best historical training weights: t1_dp0.25/cifar.best.train.w\u001b[0m\n",
      "Validating, loss=1.234: 100%|███████| 6400/6400 [00:00<00:00, 11209.60samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-01 07:36:01,972 kur.model.executor:197]\u001b[0m Validation loss: 1.234\u001b[0m\n",
      "Completed 20 epochs.\n",
      "\u001b[1;37m[INFO 2017-03-01 07:36:01,976 kur.model.executor:235]\u001b[0m Saving most recent weights: t1_dp0.25/cifar.last.w\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!kur -v train dlnd_p2_dropout.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss_batch       training_loss_batch    validation_loss_labels\r\n",
      "batch_loss_labels      training_loss_labels   validation_loss_total\r\n",
      "batch_loss_total       training_loss_total\r\n",
      "summary.yml            validation_loss_batch\r\n"
     ]
    }
   ],
   "source": [
    "# %cd t1_dp0.25/\n",
    "!ls cifar-log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Cell magic `%%pycat` not found (But line magic `%pycat` exists, did you mean that instead?).\n"
     ]
    }
   ],
   "source": [
    "%pycat summary.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.08002758,  1.80103576,  1.64649773,  1.53694201,  1.47005808,\n",
       "        1.4060986 ,  1.37083662,  1.32995439,  1.3027122 ,  1.29357958,\n",
       "        1.25282907,  1.23259354,  1.21278262,  1.19201612,  1.17865086,\n",
       "        1.15514135,  1.14136755,  1.1234889 ,  1.1214298 ,  1.10675633], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kur.loggers import BinaryLogger\n",
    "training_loss = BinaryLogger.load_column('cifar-log', 'training_loss_total') \n",
    "validation_loss = BinaryLogger.load_column('cifar-log', 'validation_loss_total') \n",
    " \n",
    "training_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl81NW9+P/XmT37RhISQhLCEpYQIAYqrrhUQa3WpVUK\n7ha1m/6sfqXlVr1eudfb67XUWrVqxdpLQetSW8WlLmiRiAKyQwBDAiEhCVnIPslkzu+PGcYsM8lA\nMplJ8n4+HvPI5DPn85l3Pgyf95zlc47SWiOEEEIAGIIdgBBCiNAhSUEIIYSHJAUhhBAekhSEEEJ4\nSFIQQgjhIUlBCCGEhyQFIYQQHpIUhBBCeEhSEEII4WEKdgAna9SoUTozMzPYYQghxJCyefPmY1rr\nxL7KDbmkkJmZyaZNm4IdhhBCDClKqRJ/yknzkRBCCA9JCkIIITwkKQghhPAYcn0KQojB0d7eTmlp\nKa2trcEORZwEm81GWloaZrP5lPaXpCCE8Kq0tJSoqCgyMzNRSgU7HOEHrTXV1dWUlpYybty4UzrG\niGg+WlVRQWZBAYZ168gsKGBVRUWwQxIi5LW2tpKQkCAJYQhRSpGQkNCv2t2wrymsqqhgSWEhzU4n\nACV2O0sKCwFYlJwczNCECHmSEIae/v6bDfuawrKiIk9COKHZ6WRZUVGQIhJCiNA17JPCIbv9pLYL\nIYKvurqamTNnMnPmTEaPHs2YMWM8v7e1tfl1jJtvvplCd6uAL7///e9ZtWrVQITMWWedxdatWwfk\nWME07JuP0q1WSrwkgHSrNQjRCDF8raqoYFlREYfsdtKtVpZnZZ1yE21CQoLnAvvQQw8RGRnJvffe\n26WM1hqtNQaD9++2K1eu7PN9fvzjH59SfMPZsK8pLM/KIrzbhybcYGB5VlaQIhJi+DnRd1dit6P5\npu9uoAd1HDhwgJycHO644w7y8vIoLy9nyZIl5OfnM23aNB5++GFP2RPf3B0OB7GxsSxdupQZM2Yw\nd+5cKisrAfi3f/s3VqxY4Sm/dOlS5syZQ3Z2Nhs2bACgqamJq6++mhkzZrBw4ULy8/P9rhG0tLRw\n4403Mn36dPLy8vj0008B2LFjB7Nnz2bmzJnk5uZSVFREQ0MDCxYsYMaMGeTk5PDqq68O5Knz27Cv\nKZz4prKsqMhTY/jPceOkk1mIk3D3/v1sbWz0+frn9fXYte6yrdnp5Na9e3murMzrPjMjI1kxceJJ\nx7J7925WrlzJM888A8Cjjz5KfHw8DoeD8847j2uuuYapU6d22ef48eOce+65PProo9xzzz288MIL\nLF26tMextdZ88cUX/P3vf+fhhx/m3Xff5Xe/+x2jR4/mtddeY9u2beTl5fkd6xNPPIHFYmHHjh3s\n2rWLSy65hP379/PUU09x7733cu2112K329Fa8+abb5KZmck777zjiTkYhn1NAVyJoXjuXHbPng2A\nzWgMckRCDC/dE0Jf2/tj/PjxzHb/XwZYvXo1eXl55OXlsWfPHnbv3t1jn7CwMBYsWADAaaedRnFx\nsddjX3XVVT3KrF+/nuuuuw6AGTNmMG3aNL9jXb9+Pddffz0A06ZNIzU1lQMHDnDGGWfwyCOP8Otf\n/5rDhw9js9nIzc3l3XffZenSpXz22WfExMT4/T4DadjXFDqbHB5OutXKuzU13J6aGuxwhBgy+vpG\nn1lQ4LXvLsNqZd2sWQMaS0REhOf5/v37+e1vf8sXX3xBbGwsixcv9jpG32KxeJ4bjUYcDofXY1vd\nfY2dy+h+JDZf+15//fXMnTuXt99+m29/+9v86U9/4pxzzmHTpk2sXbuW++67j8suu4xf/vKXp/ze\np2pE1BROUEqxID6eD2praes2TFUIceqC1XdXX19PVFQU0dHRlJeX89577w34e5x11lm88sorgKsv\nwFtNxJdzzjnHM7ppz549lJeXM2HCBIqKipgwYQJ33XUXl156Kdu3b+fIkSNERkZy/fXXc88997Bl\ny5YB/1v8MaJqCgDz4+P5Q3k5G44fZ15cXLDDEWJY6Nx3NxCjj/yVl5fH1KlTycnJISsrizPPPHPA\n3+OnP/0pN9xwA7m5ueTl5ZGTk+Ozaefiiy/2zDl09tln88ILL3D77bczffp0zGYzL730EhaLhb/8\n5S+sXr0as9lMamoqjzzyCBs2bGDp0qUYDAYsFounz2Swqf5UjYIhPz9f92eRnXqHg4TPPuPnaWk8\nOn78AEYmxPCyZ88epkyZEuwwgs7hcOBwOLDZbOzfv5+LLrqI/fv3YzKF7ndqb/92SqnNWuv8vvYN\n3b8qQKJNJs6KieGdmhpJCkKIPjU2NnLBBRfgcDjQWvOHP/whpBNCfw3fv6wX8+PjWVpURJndTqrc\nxCaE6EVsbCybN28OdhiDZkR1NJ8wPz4egPdqaoIciRBChJYRmRRyIyJIsVh4R5KCEEJ0EbCkoJQa\nq5T6WCm1Rym1Syl1l5cySin1hFLqgFJqu1LK/1sF+xcb8+Pj+WdtLQ4ZmiqEEB6BrCk4gJ9rracA\npwM/VkpN7VZmATDR/VgCPB3AeLq+cXw8dQ4HGxsaBusthRAi5AUsKWity7XWW9zPG4A9wJhuxa4A\nXtIunwOxSqmUQMXU2YVxcRiAd6UJSYiQM2/evB43oq1YsYIf/ehHve4XGRkJQFlZGddcc43PY/c1\nrH3FihU0Nzd7fr/kkkuoq6vzJ/RePfTQQzz22GP9Pk4gDUqfglIqE5gFbOz20hjgcKffS+mZOFBK\nLVFKbVJKbaqqqhqQmOLMZk6PjpakIMQAqVhVQUFmAesM6yjILKBi1anPkLpw4ULWrFnTZduaNWtY\nuHChX/unpqb2a5bR7klh7dq1xMbGnvLxhpKAJwWlVCTwGnC31rq++8tedulxN53W+lmtdb7WOj8x\nMXHAYlsQH8+mhgYq/Vy0QwjhXcWqCgqXFGIvsYMGe4mdwiWFp5wYrrnmGt566y3s7vmUiouLKSsr\n46yzzvLcN5CXl8f06dN58803e+xfXFxMTk4O4Jq++rrrriM3N5drr72WlpYWT7k777zTM+32gw8+\nCLhmNi0rK+O8887jvPPOAyAzM5Njx44B8Pjjj5OTk0NOTo5n2u3i4mKmTJnCD3/4Q6ZNm8ZFF13U\n5X364u2YTU1NXHrppZ6ptF9++WUAli5dytSpU8nNze2xxsRACOh9CkopM66EsEpr/bqXIqXA2E6/\npwHe59kNgPnx8fyquJj3a2pYPHr0YL2tEEPO/rv307jV99TZ9Z/Xo+1dv885m53svXUvZc95/y8d\nOTOSiSu8T7SXkJDAnDlzePfdd7niiitYs2YN1157LUopbDYbb7zxBtHR0Rw7dozTTz+dyy+/3Ofa\nxE8//TTh4eFs376d7du3d5n6evny5cTHx9PR0cEFF1zA9u3b+dnPfsbjjz/Oxx9/zKhRo7oca/Pm\nzaxcuZKNGzeiteZb3/oW5557LnFxcezfv5/Vq1fz3HPP8f3vf5/XXnuNxYsX+zxnfR2zqKiI1NRU\n3n77bcA1lXZNTQ1vvPEGe/fuRSk1IE1a3QVy9JEC/gjs0Vo/7qPY34Eb3KOQTgeOa63LAxVTd3lR\nUSSazdKEJEQ/dU8IfW33R+cmpM5NR1prfvnLX5Kbm8uFF17IkSNHqOhlMZ9PP/3Uc3HOzc0lNzfX\n89orr7xCXl4es2bNYteuXX1Odrd+/XquvPJKIiIiiIyM5KqrruJf//oXAOPGjWPmzJlA79Nz+3vM\n6dOn88EHH3D//ffzr3/9i5iYGKKjo7HZbNx22228/vrrhIeH+/UeJyOQNYUzgeuBHUqpE8sU/RJI\nB9BaPwOsBS4BDgDNwM0BjKcHg1JcHB/PuzU1OLXG4OObhhAjna9v9CcUZBa4mo66sWZYmbXu1KbO\n/u53v+uZLbSlpcXzDX/VqlVUVVWxefNmzGYzmZmZXqfL7sxbLeLgwYM89thjfPnll8TFxXHTTTf1\neZze5oqzdpodwWg0+t185OuYkyZNYvPmzaxdu5Zf/OIXXHTRRTzwwAN88cUXfPjhh6xZs4Ynn3yS\njz76yK/38VcgRx+t11orrXWu1nqm+7FWa/2MOyHgHnX0Y631eK31dK31qc90d4rmx8dzrL2dzTI0\nVYhTlrU8C0N418uJIdxA1vJTnzo7MjKSefPmccstt3TpYD5+/DhJSUmYzWY+/vhjSkpKej1O5+mr\nd+7cyfbt2wHXtNsRERHExMRQUVHhWfEMICoqigYv14RzzjmHv/3tbzQ3N9PU1MQbb7zB2Weffcp/\nY2/HLCsrIzw8nMWLF3PvvfeyZcsWGhsbOX78OJdccgkrVqzwe1nQkzEi5z7q7KK4OBSuoamzo6OD\nHY4QQ1LyItcU2UXLirAfsmNNt5K1PMuz/VQtXLiQq666qstIpEWLFvGd73yH/Px8Zs6cyeTJk3s9\nxp133snNN99Mbm4uM2fOZM6cOYBrFbVZs2Yxbdq0HtNuL1myhAULFpCSksLHH3/s2Z6Xl8dNN93k\nOcZtt93GrFmz/G4qAnjkkUc8nckApaWlXo/53nvvcd9992EwGDCbzTz99NM0NDRwxRVX0Nraitaa\n3/zmN36/r79G3NTZ3szZvBmTUmw4ibVXhRjuZOrsoas/U2ePyLmPupsfH8/G+npq2tuDHYoQQgSV\nJAVc9ys4gX/W1gY7FCGECCpJCsDsqCjiTCYZmipEN0OteVn0/99MkgJgMhj4dlwc79bUyH8CIdxs\nNhvV1dXyf2II0VpTXV2NzWY75WOM+NFHJyyIj+eVqiq2NTYyMyoq2OEIEXRpaWmUlpYyUPONicFh\ns9lIS0s75f0lKbhd7F6N7d2aGkkKQgBms5lx48YFOwwxyKT5yC3FamVGRIT0KwghRjRJCp0sSEjg\ns/p66h2OYIcihBBBIUmhk/nx8Ti05kMZmiqEGKEkKXRyRnQ0UUYj70gTkhBihJKk0InZYOBCGZoq\nhBjBJCl0Mz8+nsN2O3s6LcUnhBAjhSSFbua7h6ZKE5IQYiSSpNBNus3G1PBwGZoqhBiRJCl4MT8+\nnk/r6mjq6Ah2KEIIMahGRFKoWFVBQWYB6wzrKMgsoGKV7/VcwTXlRZvWfCxDU4UQI8ywTwoVqyoo\nXFLoWj9Wg73ETuGSwl4Tw9mxsYQbDNKEJIQYcYZ9UihaVoSz2dllm7PZSdGyIp/7WA0Gzo+L4x0Z\nmiqEGGGGfVKwH7Kf1PYT5sfHU9TayoGWlkCEJYQQIWnYJwVruvWktp8wv9OsqUIIMVIM+6SQtTwL\nQ3jXP9NgM5C1PKvX/caHhTExLEzuVxBCjCjDPikkL0om+9lsrBlWUK5t0edGk7wouc9958fHs66u\njhYZmiqEGCGGfVIAV2KYWzyXec55JH4/kcYvG3HanX3uNz8+nhank38dPz4IUQohRPCNiKTQWcqt\nKThqHBz727E+y86LjcWqlDQhCSFGjBGXFOIujMOaYaX8+fI+y4YbjZwbGyudzUKIEWPEJQVlUKTc\nkkLtB7W0HOx7uOmC+Hj2NjdTLENThRAjwIhLCgCjbx4NCo6uPNpnWRmaKoQYSUZkUrCNtRE/P57y\nF8rRHb3fsZwdHk6G1SpJQQgxIozIpACuDue2I23UvNf7xV4pxYKEBD6sq6PN2feIJSGEGMoClhSU\nUi8opSqVUjt9vB6jlPqHUmqbUmqXUurmQMXiTcJ3EjAnmv3qcJ4fH09jRwefydBUIcQwF8iawovA\n/F5e/zGwW2s9A5gH/K9SyhLAeLowWAyMvnE01f+opq2irdeyx9pcr5+/bRuZBQWsquh96m0hhBiq\nApYUtNafAr21zWggSimlgEh3WUeg4vFm9K2j0Q7N0Zd8dzivqqjgZwcOeH4vsdtZUlgoiUEIMSwF\ns0/hSWAKUAbsAO7SWg9qo33E5Aiiz4ym/Plyn1NkLysqorlbX0Kz08myIt9TbwshxFAVzKRwMbAV\nSAVmAk8qpaK9FVRKLVFKbVJKbaqqqhrQIFJuS6FlXwvH13vvLzhk9z7Ftq/tQggxlAUzKdwMvK5d\nDgAHgcneCmqtn9Va52ut8xMTEwc0iKTvJWGMMvrscE63ep9i29d2IYQYyoKZFA4BFwAopZKBbGDQ\n22SMEUaSfpBE1V+rcBzv2aWxPCuLcEPX02RybxdCiOEmkENSVwMFQLZSqlQpdatS6g6l1B3uIv8B\nnKGU2gF8CNyvte57lroASLktBWeLk4rVPTuPFyUn82x2NhlWKwqIMhrpAOZGe23pEkKIIU0NtTWI\n8/Pz9aZNmwb0mFprNs3chDIr8jfl91q2zG5n/MaNXJuYyItTpgxoHEIIEShKqc1a694vcIzgO5o7\nU0qRclsKjZsbadja0GvZVKuVO1NT+XNFBfuamwcpQiGEGBySFNySFyWjrIqjf+x7krz709OxGQz8\ne3Fx4AMTQohBJEnBzRxvJvHqRCr+r4KOlt6X30y2WPjJmDGsrqxkV1PTIEUohBCBJ0mhk5TbUnDU\nOTj2et/93feNHUuE0Si1BSHEsCJJoZPYc2OxZdn8miRvlMXC3Wlp/LWqim2NjYMQnRBCBJ4khU6U\nQZFyawp16+poPtB3J/I9aWnEGI08JLUFIcQwIUmhm9E3jQYDHH2h7w7nOLOZe8aO5W/HjrG5ofdR\nS0IIMRRIUujGmmol4dIEjq48itPR9/x8d6elEWcy8cDBg4MQnRBCBJYkBS9Sbk2h7WgbNWv7XoIz\n2mTivrFjWVtTw+eyCI8QYoiTpOBF/CXxWEZb/OpwBvjpmDEkms08IH0LQoghTpKCFwazgdE3jaZ6\nbTX2sr6nyI40mbg/PZ1/1tbyr7q6QYhQCCECQ5KCD6NvGQ0dcPRPfXc4A9yZmspoi0VqC0KIIU2S\ngg/hE8OJnRdL+R/L0c6+Jw0MNxr5RXo66+rq+Ki2dhAiFEKIgSdJoRejbx1N69et1H3iX5PQkpQU\nxlgs/OrgQZ/LewohRCiTpNCLxKsTMcYYKf+jfx3ONqORZRkZbKiv532pLQghhiBJCr0whhlJXpxM\n1atVtNe2+7XPrSkpZFitUlsQQgxJkhT6kHJbCtquqVjVc1U2bywGA7/KzOTLhgbeqq4OcHRCCDGw\nJCn0IWpmFJF5kZQ/X+73N/8bkpPJstl4oLhYagtCiCFFkoIfInIjaNrWxCfGTyjILOiz1mA2GHgw\nM5OtjY28cSwoy04LIcQpkaTQh4pVFVS9XOX6RYO9xE7hksI+E8MPkpLIDgvjweJinFJbEEIMEZIU\n+lC0rAhnS9eJ8ZzNToqWFfW6n8ldW9jZ1MRfq6oCGaIQQgwYSQp9sB/yPs2Fr+2dXZuUxLTwcB4q\nLqZDagtCiCFAkkIfrOnWk9remUEp/n3cOPY2N7O6wr/RS0IIEUySFPqQtTwLQ3jP0xR3UZxf+185\nahRjLRZuLizEsG4dmQUFrJIEIYQIUZIU+pC8KJnsZ7OxZlhBuWoI4TnhVLxY4df0F6srK6lsb8eh\nNRoosdtZUlgoiUEIEZLUUBtHn5+frzdt2hTUGNrr2vlq7le0VbaRtzGP8AnhPstmFhRQYu/Z/5Bh\ntVI8d24gwxRCCA+l1GatdX5f5aSmcArMsWamvzUdFOy4bEevU2Ac8pIQetsuhBDBJEnhFIWNDyPn\n9Rxai1rZ/f3dONu9r+ecbvXeIe1ruxBCBJNfSUEpNV4pZXU/n6eU+plSKjawoYW+2HNimfSHSdR+\nUMuBuw54ndJieVYW4Yaup1kBv0hPH6QohRDCf/7WFF4DOpRSE4A/AuOAvwQsqiEk5eYUxv6/sZQ9\nXcaRJ4/0eH1RcjLPZmeTYbWigBSLBQPwTk2NzIskhAg5Jj/LObXWDqXUlcAKrfXvlFJfBTKwoSTr\nv7Jo2dfCgbsPEDYhjIQFCV1eX5SczKLkZM/vvzl8mHu+/po/lJVxx5gxgx2uEEL45G9NoV0ptRC4\nEXjLvc0cmJCGHmVQTP7zZCJzI9l97W6adjX1Wv6utDQujovj//v6a3Y39V5WCCEGk79J4WZgLrBc\na31QKTUO+L/edlBKvaCUqlRK7eylzDyl1Fal1C6l1Cf+hx16TJEmcv6RgzHCyI7LdtBW1eazrEEp\n/jRlCtFGI9ft3k1rR8cgRiqEEL75lRS01ru11j/TWq9WSsUBUVrrR/vY7UVgvq8X3R3VTwGXa62n\nAd/zM+aQZUuzkfP3HNqOtrHzyp047d5HJAEkWyysnDyZHU1N3F/U++R6QggxWPwdfbROKRWtlIoH\ntgErlVKP97aP1vpToKaXIj8AXtdaH3KXr/Qz5pAWPTuayS9Npv6zegp/WNhrZ/IlCQncNWYMTxw5\nwtuySpsQIgT423wUo7WuB64CVmqtTwMu7Od7TwLi3Alns1Lqhn4eL2QkfS+JzIczqfhzBYcePdRr\n2UezspgREcHNe/dSLje0CSGCzN+kYFJKpQDf55uO5v4yAacBlwIXA79SSk3yVlAptUQptUkptalq\niKxNkPFvGST9IImDvzxI1Wu+Y7YZjfxl6lQaOzq4ae9eWZBHCBFU/iaFh4H3gK+11l8qpbKA/f18\n71LgXa11k9b6GPApMMNbQa31s1rrfK11fmJiYj/fdnAopcj+YzbRc6PZtXAXG1I2sM6wzutynlMj\nIvjNhAm8X1vLitLSIEUshBD+dzT/VWudq7W+0/17kdb66n6+95vA2Uopk1IqHPgWsKefxwwpRpuR\n5MXJ4IC2o229Lue5JCWF744axdKiIrY0NAQpYiHESOdvR3OaUuoN9xDTCqXUa0qptD72WQ0UANlK\nqVKl1K1KqTuUUncAaK33AO8C24EvgOe11j6Hrw5Vh359CLq1CHlbzlMpxfPZ2SSZzSzcvZsmGaYq\nhAgCf+9oXolrWosTw0YXu7d929cOWuuFfR1Ua/0/wP/4GcOQdDLLeSaYzfx5yhQu2LaNuw8c4Lns\n7ECHJ4QQXfjbp5CotV6ptXa4Hy8CQ6NxP8h8Luc51vv28+LiWJqezvPl5bxaOSxG6QohhhB/k8Ix\npdRipZTR/VgMyMB6P/hazjMyP9LnPv+emcmcqCh+uG8fh1pbAxmeEEJ04W9SuAXXcNSjQDlwDa6p\nL0QfvC3nGX1uNNWvV1P2XJnXfcwGA3+ZOhWH1izes4cOGaYqhBgkfvUpuO86vrzzNqXU3cCKQAQ1\n3CQvSiZ50TezpDrbnez87k723bEPy2gLo74zqsc+48PCeGriRG7Yu5f/LCnhV5mZgxixEGKk6s/K\na/cMWBQjjMFsYNor04g6LYrd1+7m+OfHvZZbnJzMD5KS+PfiYjYc915GCCEGUn+SghqwKEYgY4SR\n6W9Nx5JqYcdlO2je19yjjFKKpyZNIs5k4pyvvsKwbh2ZBQWsqqjwckQhhOi//iQFaejuJ0uShdx3\nc1EGxfb527Ef7TlM9a3qaho6OujAdcJL7HaWFBZKYhBCBESvSUEp1aCUqvfyaABSBynGYS18QjjT\n355OW0UbOy7dgaPB0eX1ZUVF2Lt1NDc7nSyT6baFEAHQa1LQWkdpraO9PKK01v7e+Cb6ED07mmmv\nTqNxWyO7rtmFs+2bdRgO+Zg51dd2IYToj/40H4kBlLAggeznsql9v5bC275ZhyHd6v0mN5NSlMg9\nDEKIASZJIYSk3JxC5n+41mE4uOwgAMuzsgg3dP1nsiqFCZizeTOfy6gkIcQAkqQQYjKWZZByewqH\n/usQR35/hEXJyTybnU2G1YoCMqxW/jh5Mpvz84k0Gpm3dSsvy3QYQogBIv0CIUYpxcQnJ9JW3sb+\nn+7Hkmph0ZXJLEpO7lF2Y14e3925k+t272Z/czPLMjJQSkYKCyFOndQUQpDBZGDq6qlEfyua3Qt3\nU7e+zmu5URYLH86cyeLkZH5VXMyNe/didzq9lhVCCH9ITSFEGcON5Pwjh6/O/IptF2/DHGOm7Wgb\n1nQrWcuzPNNmWA0GXpo8mUlhYTxQXMzB1lbemDaNURZLkP8CIcRQJDWFEGYZZSH1zlR0s6at3PfK\nbUopfpWZyZqpU/myvp5vbdnC3qamIEYuhBiqJCmEuNIVPdds9rZyG8C1SUmsmzmTxo4OTt+yhQ9q\nagYjRCHEMCJJIcSdzMptAKfHxLAxL4+xNhvzt2/n2TLv03MLIYQ30qcQ4qzpVuwlPROAdYz3m9oA\nMsPC+GzWLK7dvZvb9+3jzaoqdjY3c9huJ91qZXlWltfRTEIIITWFEOdr5TatNG3H2nzuF20y8Y+c\nHL4dG8va2loO2e0yoZ4Qok+SFEJcj5XbMqyMvX8sjioH287fRluV78RgMhjY19LSY7tMqCeE8EWa\nj4aA7iu3AcRdGMfOy3ey7fxtzPhwBpYk70NQZUI9IcTJkJrCEBV/YTzT35pOy9ctbD1vK20V3msM\nvibUizPJ9wEhRE+SFIawuPPjmL52Oq3FrWydtxV7ec9v/94m1DMANQ4Hd+7bR5vcAS2E6ESSwhAX\nNy+O3HdyaT3sTgxlXRODtwn1Xpw8mf83dizPlJVx4bZtVLb57pcQQowsSuuhtapmfn6+3rRpU7DD\nCDl16+vYsWAHlhQLMz+e2euQ1RNWV1RwS2EhiWYzf8vJIS8qahAiFUIEg1Jqs9Y6v69yUlMYJmLP\niiX3vVzajraxdd5WWkv7XoBnYXIyn82aBcCZX33FahmmKsSIJ0lhGIk5I4bc93Npq2xj67lbaT3U\nd2LIi4pi02mnMTsqih/s2cP9X39NxxCrPQohBo4khWEm5vQYZvxzBu3V7a4aQ0nfiSHJYuGDGTO4\nMzWVXx8+zGU7dlDX3j4I0QohQo0khWEoek40Mz6YgaPWwVfnfkXLwZ43sHVnMRh4atIk/jBpEh/W\n1jJnyxb2yEyrQow4khSGqeh8V2LoqO9gU/4mNozZwDrDOgoyC7pMu93dktRUPpoxg+MOB9/asoV/\nHDs2iFELIYJNksIwFnVaFGk/T6OjpoO2Mt/rMXR3Vmwsm047jYlhYVyxcyf/WVLCqqNHySwowLBu\nHZkFBTJ3khDDVMCSglLqBaVUpVJqZx/lZiulOpRS1wQqlpGs/LnyHtt8rcfQ2VibjfWzZvGDpCSW\nHTzIjXv5l/zFAAAYn0lEQVT3UiKT6gkx7AWypvAiML+3AkopI/DfwHsBjGNE87keQ4md8pXltNf5\n7lAOMxr585QpxJpMdHR7TSbVE2J4ClhS0Fp/CvS19NdPgdeAykDFMdJZ033cxGaCwlsK2ZC8gZ1X\n7qTylUo6mrtf+l1LfR53OLweQibVE2L4CVqfglJqDHAl8IwfZZcopTYppTZVVVUFPrhhxNt6DIZw\nA5NfnEzexjzG/GgM9Rvr2X3tbjYkb2D34t1Ur63G2f7NnEi+JtVTwH+VlFAjw1eFGDYCOs2FUioT\neEtrnePltb8C/6u1/lwp9aK73Kt9HVOmuTh5FasqKFpWhP2QHWu6lazlWV2m4tYdmrpP66hcXUnV\nq1U4ah2YEkwkXpNI8sJk3p7Uyqqn9nLDc5BUCZVJ8OJtcOTycHY2NxNuMHDT6NHcnZbGxPDwIP6l\nQghf/J3mIphJ4SCuL5sAo4BmYInW+m+9HVOSQmA525zUvFdD5epKjr15DGezE2OsEUdjB6pTK5Iz\nTDHtuclUXhHBb0pLWVVRQbvWXJ6QwM/HjuWsmBiUUr7fSAgxqEI+KXQr9yJSUwg5HU0dHPvHMQpv\nKcTZ0nOKbdMoE6dtPA3bOBsVbW38vqyMp48codrhID8qip+npXF1YiJmg4FVFRUsKyrikKwTLURQ\nBD0pKKVWA/Nw1QIqgAcBM4DW+pluZV9EkkLIWmdYB718TEwJJqLyo4ieE40lL4K1GS081naUfS0t\njLVaOSs6mro1VV2an176ISz6yRRJDEIMkqAnhUCRpDD4CjILsJf0HGlkSbGQ+WAm9V/W0/BlA007\nm8BdobCkWWmaYeWTrDaOHmvlmtfB1ukQrVZ4camRNQ+dPUh/hRAjm79JQdZkFH3KWp5F4ZJCnM3f\nNCEZwg2M/5/xJC9KJvX2VMDV3NTwVQMNX7oexi8buPBt7xPy2ezw3Wc64KHB+AuEEP6SpCD6dGKk\nUm8jmACMEUZiz4ol9qxYz7b2mnbWJ3yGty7npEqoa28n1mwOZPhCiJMgSUH4JXlRco8k4A9zvJmO\nNBOm0p43wGkFP7mvgOl3juUnE9KJMBoHIlQhRD/IhHgi4KY/OhFnWNe6grYqTBOt3PZbJ+PPLOFH\nP/+M3+4vobWj513VQojBI0lBBFzyomSmPTcZa4YVFFgzrEz942TO3TuXGR/PIHFKJDf/1kna6Qf5\n8c838NzXpbQ7ew6B7Y+KVRUUZBb4NX24ECOZjD4SIaF2XS1bf3UAtb6J6nh4/wYTZ/8si4WZKRj7\neRNcxaoKrx3l2c9mn1KTmBBDkQxJFUNS7bpatvzqAEZ3cvjoRjPn3z0B3qpDP1xOfCXUJIHhgVSu\n+tGkHvs7HU7sh+20FrXScrCF1oOtlK4o7ZIQTrBmWJlbPHcw/iwhgk6SghjSatbVsunf9mP5rJmG\nCNcQVnOnvuo2C7QtimX6hDhaD7bSUuRKAK2HWuk8z7cyKbTD92d89s7ZREyLCOBfIkRokKQghoWa\nT2rZ9O1tWHqZiNWcbCYsKwzbOJvn54nnljEWNk7Y6PXmuxMiZ0WSfH0ySQuTsI72MdW4EEOc3Lwm\nhoX4c+MweV/OAQ38a+dYvp85mskRvr/t+7z57rHx6HZNxZ8r+Pqer/n6vq+Jvyie5OuTGXXFKIzh\nMkRWjDySFETIq0mCUV4GC1Umw0NVh3mw6jAzIyO5LimJ65KSyLDZupTr6+a7tJ+l0bSniYo/V1Dx\nfxXs+cEejFFG19Th1ycTe24slasr+7x5T4jhQJqPRMh7/al9hN9T1mPupObHU5l7awavVFWxprKS\nz+vrAZgbHc11SUl8PzGR0e4FgvydpVU7NXWf1FHx5wqqXq2io6EDY7wRZ72zS9+EjF4SQ430KYhh\n5fWn9uF8uKzX0UcHW1p4ubKSNZWVbGtqwgDMi41lnM3GXyorael070O4wcCz2dm9ztLa0dzBsTfd\nU4e39hy9ZE4yM2fPHMzxMk2HCH2SFMSItrupiZcrK1ldWcn+lhavZVIsFnbOnk2cydTrgkB9TR0e\nNiGMqNlRRM1xTR8eOSsSY1jX/oi+Vr8TItAkKQgBaK0xfvJJb9d0IgwG0m02xlqt3/zs9Lxk0hZM\nR3r2dneMMjLh5+muWWG/aMBe6m7fMkLk9Eii5kQRNTuK9qp2Sh4pkZvnRFDJ6CMhAKUU6VYrJfae\nQ1JHmUz8MiODQ3Y7h1pbOWy3s+3YMSrau45/veAmuPcxL+tB/BjWLM3wbLOX22n4soH6L+pp+KKB\nqleqKH+23GtczmYnRcuKJCmIkCNJQQx7y7OyWFJYSHO3PoUVEyd67VNo7ejgSFsbh1pbOWS3cxN7\nAbjt+W9Wjnv+NvhoXgdrOu1nTbFivdzKqMtHAa5aSsuBFr6Y9IXXuOwldpp2NRE+NVzWsxYhQ5qP\nxIjQnzWiMwsKvNY0TErxf1Om8L3ERAy9XNR9rVx3Qlh2GInXJJJ4dSKRMyMlQYiAkD4FIQbIqoqK\nHjUNq1IkmEyUtbeTGxHBf4wbx3cSErxe0H1NyJf16yyUQVH1ahV1n9RBB9iybCRelUjiNYlEzY5C\nGZTnGMHsqA72+4v+k6QgxADyVtO4LimJlysrebC4mAMtLcyJiuKRceO4MC6uR3Lo66LadqyN6jer\nqXqtitoPatHtGmualVFXjcIYbaT08dJ+dVT356Ius8wOD5IUhBgk7U4nL1VU8HBxMYfsds6JieGR\nceM4Oza27529Ha+unep/uBJEzbs1aLv3/6PmJDPT35qOKc6EKdaEKcaEwdxziZSTuahrrXHanXTU\nd9DR0IGj3sH2Bdtpr+g5+ZTMMju0SFIQYpDZnU6eLy/nkZISjra1cXFcHP8xbhyzo6NPuU/D0eBg\nfcz6Xu+T6MwQYXAliE6Puo/rvE4dbrAZiDwtko6GDjrqXQmgo6ED3e7/NWHaG9OIPj1aJhIcAiQp\nCBEkzR0dPHXkCI8eOkS1w0FeRAS7W1poPck7qk/w1VFtTjaT/Xw2jjoHjlqH66eXR+OWRp/Hjj0/\nFlO0CWOUEWO0EVOUyfWz07Z9t++jvbKXaWoBW6aN6NOjiZ4bTfTp0UTOjMRg+abW0t8+CenT6D9J\nCkIEWYPDwW9LS3mguNjrF/0Mq5XiuX03v/S3Td9XUvG3+cfX+0/8/UTCJ4VT/3m961FQ77mBT1kV\nUXlRRM+Nxtnu5OjzR3G2nFr80qcxMCQpCBEiDOvW+Wz9eS83lzNjYogw9j5Nd7A7iv19/9bSVho2\nNnC84Dj1n9fTsKnBZ5+IMivCJoV983vnzvlOT5v3Nntt0rKMsTD38NxBGcI7HGoqkhSECBG+7nM4\nwawUp0dHc15sLOfHxXF6dDRWQ9cO4/7cZwHBu6g525x8avvUZ5/IqKtdN/p1eb3b82N/O+bz+KZ4\nE5EzI7s8wieH9+hwD3ZS7a+B+PeTpCBEiPB2n0O4wcDvJkxgjM3GR7W1fFRXx5aGBpyAzWDgzOho\nzo+L4/zYWPY3N3PH/v099ve3TyLY+tt85Wt/U7yJxGsSadzaSNP2Js9MtsqiiMiJ8CSJtqo2Sh8r\n9dp8lbQwCWeLk47mDjqaOnA2u547m9w/m50U3l6Io7rn3FeDNfpqoJKSJAUhQog/3/Tr2tv59Phx\nPq6r46PaWrY3NQGulpT+9EkEW38vav7s73Q4adnfQuPWxm8eXzXSXtVLB7mvE3sSkhcnEzE9wvOw\njrH6vIHxZL7pO9ud2EvttBa3sut7uwYkKUlSEGKIq2pr45O6Or63e7fPMltOO43cyEiMIT41RjBG\nH2mtaTvaRsGYAp8X/4wHMjBGGDGEGzCGG7953mnb9gXbaStr67GvwWbAlGCi7cg3r5liTUTkfJMk\nInIiaNrbxNd3f90jqY1bPo7IGZG0Frf2eNhL7dBzFHFXCuY55/VRqFNxSQpCDA999UnEmkycExPD\nebGxzIuNJTcyssdcTP3tkxjKAjX66kRNpb2mnaZdTTTtcD0adzTStLOJjuMd/gepwJpmxZZp6/HY\nc/0er0kpUDUFmSVViBDna5bXR8eNI95iYV1dHevq6vh7dTUAcSeSRFwc82Jj2dHYyO379nn2L7Hb\nWVJYCDAiEkPW8izvc08tz/Jr/77W+DbHm4k9O5bYs7+5g11rjb3UTtOOJnZcusPnsWd8OANbpg1r\nmrXLfR2djf/1+H7Ff7ICVlNQSr0AXAZUaq1zvLy+CLjf/WsjcKfWeltfx5WaghiJ/Pmmf7i1lU/c\nCeLjujqKWlsBMOC9JWKo9EkMhGAOKe1vTQWGyegjpdQ5uC72L/lICmcAe7TWtUqpBcBDWutv9XVc\nSQpC+OeQO0ncsHevzzIlp59Ous02iFGNPKEwpBX8Twre6ysDQGv9KVDTy+sbtNa17l8/B9ICFYsQ\nI1G6zcb1o0eTYfU9L1HG558zceNG7ty3j1crK6lu7306C3Hykhclk/1sNtYMq6vvIMMa0ndjh0qf\nwq3AO8EOQojhyFefxAMZGdiMRj6orWVVRQXPlJWhgFmRkVwQF8eFcXGcFRNDuNE4ojuqB0LyouSQ\nTQLdBXT0kVIqE3jLW/NRpzLnAU8BZ2mtq32UWQIsAUhPTz+tpKRk4IMVYhjr66Le7nSyqaGBD2pr\n+bC2lg319bRrjUUpxttsHGhtpb3TtWIo3TwnXILep+AOIpNekoJSKhd4A1igtd7nzzGlT0GIwGvq\n6GD98eN8WFvLitLSLgnhhBijkWcmTWJaRATZ4eFYDL5bo6WmEXwhPyRVKZUOvA5c729CEEIMjgij\nkYvj47k4Pp7HDh/2WuZ4RwcL9+wBXOtVTwwLY1pEBDkREUwLD2daRAQTw8J4uaqqS/PVSBsSO9QE\nLCkopVYD84BRSqlS4EHADKC1fgZ4AEgAnnLfFu7wJ4sJIQZXutXq9ea5dKuVf0yfzq6mJnY2NbGr\nqYmvGhp4rarKcwOxRSk09KhpNDudLCsqkqQQggKWFLTWC/t4/TbgtkC9vxBiYPjqqP7PrCxyIyPJ\njYzsUr65o4O9zc2eRPFrHzWNQ3Y79Q4H0aZQGe8iIHRGHwkhQtSJb/P+9gmEG43kRUWRFxUFwMuV\nlV5rGhpI+Owzzo2J4bKEBC5LSGBCeHjA/g7hH5n7SAgRUL6mDv95WhptWvNWdTW7mpsBmBwe7kkQ\nZ0ZHY3J3XktHdf+FxOijQJCkIMTQ09dFvailhberq3mrupqP6+po15pYk4kF8fHEmUysPHqUliG6\nnkSokKQghBiSGhwO/llby1vV1bxdXU2lj7us061WSkbI3E0DIeSHpAohhDdRJhNXJSZyVWIiTq0x\nffKJ1+UQDtntTP3iC6a7h8FOj4hgemQk42w2mTq8HyQpCCFClkEpn0NiY4xGJoWFsamhgVeqqjzb\nww0Gpp1IEhERVLS18dsjRzzNT3KfRO8kKQghQpqvIbG/nzTJc1FvdDjY3dzMDvc9EzsaG3mrupoX\njh71esxmp5NfyH0SXklSEEKENH+GxEaaTMyJjmZOdHSXfSvb2hi9YYPX5qfDdjunbdrEGTExnBEd\nzRkxMaRbva+xPJJIUhBChLxFycmn9K0+yWLptfkpzmRiZXk5Tx45AkCqxdIlScyKjMRiMPS7T2Io\n9WlIUhBCDGt9NT85nE52NDWxob6eDcePs6G+nlfdfRQ2g4F0q5Wi1lYc7pGaJXY7txUWUtnWxjWJ\niVgNBtdDKSwGg9dO7qE095MMSRVCDHsn+029zG6nwJ0knjxyhLaTuE6alPIkCKvBQFVbGx1eyqVa\nLByeO7dHEgkUuU9BCCEGgGHdOq99EgDPTppEm9bYnU7anE7sXp4/V17u89hRRiMzIiOZGRnJjIgI\nZkZGMi0igjCjsUu5gWh+kvsUhBBiAPjqk8iwWvlhamqf+79fU+N1/3iTiYVJSWxtbORPR4/S0OGq\nTxiB7PBwV6KIjKSmvZ0nBnFIrSQFIYToha8+ieVZWf3a/4mJEz0XdafWHGxtZVtjI1vdj/XHj/OX\nykqvxwzk1OOSFIQQohcnO0vsqexvUIrxYWGMDwvjqsREz/aa9nZGffaZzzu6A0GSghBC9OFUh8T2\nd/94s7nXRY4CwfeiqkIIIYJueVYW4d3Wvz6Z5quTJUlBCCFC2KLkZJ7NzibDakXh6uAO5LTh0nwk\nhBAhrr/NVydDagpCCCE8JCkIIYTwkKQghBDCQ5KCEEIID0kKQgghPIbchHhKqSqgJNhx+DAKOBbs\nIHoR6vFB6Mco8fWPxNc//YkvQ2ud2FehIZcUQplSapM/sxAGS6jHB6Efo8TXPxJf/wxGfNJ8JIQQ\nwkOSghBCCA9JCgPr2WAH0IdQjw9CP0aJr38kvv4JeHzSpyCEEMJDagpCCCE8JCmcJKXUWKXUx0qp\nPUqpXUqpu7yUmaeUOq6U2up+PDDIMRYrpXa437vHgtbK5Qml1AGl1HalVN4gxpbd6bxsVUrVK6Xu\n7lZm0M+fUuoFpVSlUmpnp23xSql/KqX2u3/G+dj3RneZ/UqpGwcxvv9RSu11/xu+oZSK9bFvr5+H\nAMb3kFLqSKd/x0t87DtfKVXo/jwuHcT4Xu4UW7FSaquPfQN6/nxdU4L2+dNay+MkHkAKkOd+HgXs\nA6Z2KzMPeCuIMRYDo3p5/RLgHUABpwMbgxSnETiKa/x0UM8fcA6QB+zstO3XwFL386XAf3vZLx4o\ncv+Mcz+PG6T4LgJM7uf/7S0+fz4PAYzvIeBePz4DXwNZgAXY1v3/U6Di6/b6/wIPBOP8+bqmBOvz\nJzWFk6S1Ltdab3E/bwD2AGOCG9VJuwJ4Sbt8DsQqpVKCEMcFwNda66DfjKi1/hSo6bb5CuBP7ud/\nAr7rZdeLgX9qrWu01rXAP4H5gxGf1vp9rbXD/evnQNpAv6+/fJw/f8wBDmiti7TWbcAaXOd9QPUW\nn1JKAd8HVg/0+/qjl2tKUD5/khT6QSmVCcwCNnp5ea5SaptS6h2l1LRBDQw08L5SarNSaomX18cA\nhzv9XkpwEtt1+P6PGMzzd0Ky1rocXP9xgSQvZULlXN6Cq/bnTV+fh0D6ibt56wUfzR+hcP7OBiq0\n1vt9vD5o56/bNSUonz9JCqdIKRUJvAbcrbWu7/byFlxNIjOA3wF/G+TwztRa5wELgB8rpc7p9rry\nss+gDkNTSlmAy4G/enk52OfvZITCuVwGOIBVPor09XkIlKeB8cBMoBxXE013QT9/wEJ6ryUMyvnr\n45riczcv2/p1/iQpnAKllBnXP94qrfXr3V/XWtdrrRvdz9cCZqXUqMGKT2td5v5ZCbyBq4reWSkw\nttPvaUDZ4ETnsQDYorWu6P5CsM9fJxUnmtXcPyu9lAnquXR3LF4GLNLuRubu/Pg8BITWukJr3aG1\ndgLP+XjfYJ8/E3AV8LKvMoNx/nxcU4Ly+ZOkcJLc7Y9/BPZorR/3UWa0uxxKqTm4znP1IMUXoZSK\nOvEcV2fkzm7F/g7c4B6FdDpw/EQ1dRD5/HYWzPPXzd+BE6M5bgTe9FLmPeAipVScu3nkIve2gFNK\nzQfuBy7XWjf7KOPP5yFQ8XXup7rSx/t+CUxUSo1z1x6vw3XeB8uFwF6tdam3Fwfj/PVyTQnO5y9Q\nPerD9QGchat6th3Y6n5cAtwB3OEu8xNgF66RFJ8DZwxifFnu993mjmGZe3vn+BTwe1yjPnYA+YN8\nDsNxXeRjOm0L6vnDlaDKgXZc375uBRKAD4H97p/x7rL5wPOd9r0FOOB+3DyI8R3A1Z584nP4jLts\nKrC2t8/DIMX3Z/fnazuuC1xK9/jcv1+Ca8TN14MZn3v7iyc+d53KDur56+WaEpTPn9zRLIQQwkOa\nj4QQQnhIUhBCCOEhSUEIIYSHJAUhhBAekhSEEEJ4SFIQohulVIfqOpPrgM3cqZTK7DxTpxChxhTs\nAIQIQS1a65nBDkKIYJCaghB+cs+r/99KqS/cjwnu7RlKqQ/dE799qJRKd29PVq51Dra5H2e4D2VU\nSj3nnjv/faVUWND+KCG6kaQgRE9h3ZqPru30Wr3Weg7wJLDCve1JXFOR5+KalO4J9/YngE+0a2K/\nPFx3xAJMBH6vtZ4G1AFXB/jvEcJvckezEN0opRq11pFethcD52uti9wTmB3VWicopY7hmsKh3b29\nXGs9SilVBaRpre2djpGJa/77ie7f7wfMWutHAv+XCdE3qSkIcXK0j+e+ynhj7/S8A+nbEyFEkoIQ\nJ+faTj8L3M834JrdE2ARsN79/EPgTgCllFEpFT1YQQpxquQbihA9hamui7i/q7U+MSzVqpTaiOsL\n1UL3tp8BLyil7gOqgJvd2+8CnlVK3YqrRnAnrpk6hQhZ0qcghJ/cfQr5WutjwY5FiECR5iMhhBAe\nUlMQQgjhITUFIYQQHpIUhBBCeEhSEEII4SFJQQghhIckBSGEEB6SFIQQQnj8/y952Xg23CiHAAAA\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x104351978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "epoch = list(range(1, 1+len(training_loss)))\n",
    "t_line, = plt.plot(epoch, training_loss, 'co-', label='Training Loss')\n",
    "v_line, = plt.plot(epoch, validation_loss, 'mo-', label='Validation Loss')\n",
    "plt.legend(handles=[t_line, v_line])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Tasks on Kur\n",
    "\n",
    "**Task1: Split kurfile**\n",
    "- split kurfile into fluid and default parts\n",
    "- default part remain constant\n",
    "- fluid part will be easy to change constantly for experimenting\n",
    "\n",
    "**Solution1: Split kurfile**\n",
    "- layer parameters will keep changing\n",
    "- file path will keep changing\n",
    "- data parts for train and validate will change\n",
    "- batch_size and num_batches will change\n",
    "- hyperparameters like learning rate will change\n",
    "\n",
    "**Create a dlnd_p2_defaults.yml**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dlnd_p2_defaults.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile dlnd_p2_defaults.yml\n",
    "\n",
    "---\n",
    "settings:\n",
    "\n",
    "  # Where to get the data\n",
    "  cifar: &cifar\n",
    "    url: \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "    checksum: \"6d958be074577803d12ecdefd02955f39262c83c16fe9348329d7fe0b5c001ce\"\n",
    "    path: \"~/kur\"  \n",
    "    \n",
    "  # Backend to use                                      \n",
    "  backend:\n",
    "    name: keras    \n",
    "    \n",
    "model:                             # there should be loop design for model structure flexibility\n",
    "  - input: images                       \n",
    "  - convolution:                        \n",
    "      kernels: \"{{cnn.kernels}}\"\n",
    "      size: \"{{cnn.size}}\"\n",
    "      strides: \"{{ cnn.strides }}\"\n",
    "  - activation: relu\n",
    "  - pool:\n",
    "      size: \"{{pool.size}}\"\n",
    "      strides: \"{{pool.strides}}\"\n",
    "      type: \"{{pool.type}}\"             \n",
    "  - flatten:\n",
    "  - dense: \"{{dense.out1}}\"       # 15                    \n",
    "  - dense: \"{{dense.out2}}\"       # 10\n",
    "  - activation: softmax\n",
    "    name: labels    \n",
    "    \n",
    "\n",
    "    \n",
    "train:\n",
    "  data:\n",
    "    - cifar:\n",
    "        <<: *cifar\n",
    "        parts: \"{{part.train}}\"               # only use dataset part 1 to train\n",
    "  provider:\n",
    "    batch_size: \"{{batch_size.train}}\"        # 128\n",
    "    num_batches: \"{{num_batches.train}}\"      # total batch is less than 100, so use 1000 is to select all       #####     \n",
    "    randomize: True                           # Importance: when train on small sample we still want weights to have \n",
    "                                              # generalize power. model overfit by memorizing the same data points#### \n",
    "\n",
    "  log: \"{{path.log}}\"                         # t1_dp0.25/cifar-log                   \n",
    "  epochs: \"{{num_epochs}}\"                    # 20\n",
    "  weights:\n",
    "    initial: \"{{path.initial_w}}\"             # t1_dp0.25/cifar.best.valid.w    \n",
    "    best: \"{{path.best_train_w}}\"             # t1_dp0.25/cifar.best.train.w\n",
    "    last: \"{{path.last_w}}\"                   # t1_dp0.25/cifar.last.w\n",
    "\n",
    "  optimizer:\n",
    "    name: adam\n",
    "    learning_rate: \"{{learning_rate}}\"        # 0.001       \n",
    "\n",
    "\n",
    "validate:\n",
    "  data:\n",
    "    - cifar:\n",
    "       <<: *cifar\n",
    "       parts: \"{{part.valid}}\"                # only use dataset part 5 as validation set\n",
    "  provider:\n",
    "    batch_size: \"{{batch_size.valid}}\"        # same as training, 128\n",
    "    num_batches: \"{{num_batches.valid}}\"      # the project 2 only used 5000 data points as validation set, so 50 \n",
    "  weights: \"{{path.best_valid_w}}\"            # t1_dp0.25/cifar.best.valid.w\n",
    "\n",
    "test: &test\n",
    "  data:\n",
    "    - cifar:\n",
    "       <<: *cifar\n",
    "       parts: test\n",
    "  weights: \"{{path.best_valid_w}}\"            # t1_dp0.25/cifar.best.valid.w\n",
    "  provider:\n",
    "    batch_size: \"{{batch_size.test}}\"         # same as training, 128\n",
    "    num_batches: \"{{num_batches.test}}\"      # the entire part test will be used, set it 1000\n",
    "\n",
    "evaluate:\n",
    "  <<: *test\n",
    "  destination: \"{{path.result}}\"              # t1_dp0.25/cifar.results.pkl\n",
    "\n",
    "loss:\n",
    "  - target: labels                       \n",
    "    name: categorical_crossentropy        \n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create dlnd_p2_fluid.yml**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dlnd_p2_fluid.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile dlnd_p2_fluid.yml\n",
    "\n",
    "---\n",
    "settings: \n",
    "  cnn: \n",
    "    kernels: 20              # why cannot use [20]? what if there are 2 conv layers [20, 30]\n",
    "    size: [5,5]  \n",
    "    strides: [2,2]\n",
    "  pool: \n",
    "    size: [2,2]\n",
    "    strides: [2,2]\n",
    "    type: \"max\"  \n",
    "  dense: \n",
    "    out1: 15\n",
    "    out2: 10\n",
    "  part: \n",
    "    train: 1\n",
    "    valid: 5\n",
    "    test: test\n",
    "  batch_size: \n",
    "    train: 128\n",
    "    valid: 128\n",
    "    test: 128\n",
    "  num_batches: \n",
    "    train:                # 1000 and nothing here both refer to all the batches? ###########################\n",
    "    valid: 50\n",
    "    test: \n",
    "  path: \n",
    "    log: t1_dp0.25/cifar-log\n",
    "    initial_w: t1_dp0.25/cifar.best.valid.w\n",
    "    best_train_w: t1_dp0.25/cifar.best.train.w\n",
    "    last_w: t1_dp0.25/cifar.last.w\n",
    "    best_valid_w: t1_dp0.25/cifar.best.valid.w\n",
    "    result: t1_dp0.25/cifar.results.pkl\n",
    "  learning_rate: 0.001\n",
    "  num_epochs: 1\n",
    "\n",
    "\n",
    "include: dlnd_p2_defaults.yml  # to define default setting\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- is grid search or random search for best parameters or hyperparameter is a feature kur want to implement?\n",
    "- automatically searching for best parameters like gradient descent search for local or gloabl optimal is a nice thing to have right? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;37m[INFO 2017-03-01 15:24:19,985 kur.kurfile:699]\u001b[0m Parsing source: dlnd_p2_fluid.yml, included by top-level.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 15:24:19,996 kur.kurfile:699]\u001b[0m Parsing source: dlnd_p2_defaults.yml, included by dlnd_p2_fluid.yml.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 15:24:20,010 kur.kurfile:82]\u001b[0m Parsing Kurfile...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:20,010 kur.kurfile:784]\u001b[0m Parsing Kurfile section: settings\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:20,016 kur.kurfile:784]\u001b[0m Parsing Kurfile section: train\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:20,023 kur.kurfile:784]\u001b[0m Parsing Kurfile section: validate\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:20,028 kur.kurfile:784]\u001b[0m Parsing Kurfile section: test\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:20,033 kur.kurfile:784]\u001b[0m Parsing Kurfile section: evaluate\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:20,039 kur.containers.layers.placeholder:63]\u001b[0m Using short-hand name for placeholder: images\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:20,040 kur.containers.layers.placeholder:97]\u001b[0m Placeholder \"images\" has a deferred shape.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:20,050 kur.kurfile:784]\u001b[0m Parsing Kurfile section: loss\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 15:24:20,052 kur.loggers.binary_logger:107]\u001b[0m Log does not exist. Creating path: t1_dp0.25/cifar-log\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:20,980 kur.utils.package:233]\u001b[0m File exists and passed checksum: /Users/Natsume/kur/cifar-10-python.tar.gz\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:27,155 kur.providers.batch_provider:57]\u001b[0m Batch size set to: 128\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:27,863 kur.utils.package:233]\u001b[0m File exists and passed checksum: /Users/Natsume/kur/cifar-10-python.tar.gz\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:33,548 kur.providers.batch_provider:57]\u001b[0m Batch size set to: 128\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:33,548 kur.providers.batch_provider:102]\u001b[0m Maximum number of batches set to: 50\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:33,548 kur.backend.backend:187]\u001b[0m Using backend: keras\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 15:24:33,549 kur.backend.backend:80]\u001b[0m Creating backend: keras\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 15:24:33,549 kur.backend.backend:83]\u001b[0m Backend variants: none\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 15:24:33,549 kur.backend.keras_backend:122]\u001b[0m No particular backend for Keras has been requested.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:33,549 kur.backend.keras_backend:124]\u001b[0m Using the system-default Keras backend.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:33,549 kur.backend.keras_backend:189]\u001b[0m Overriding environmental variables: {'TF_CPP_MIN_LOG_LEVEL': '1', 'KERAS_BACKEND': None, 'THEANO_FLAGS': None}\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 15:24:34,503 kur.backend.keras_backend:195]\u001b[0m Keras is loaded. The backend is: theano\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 15:24:34,503 kur.model.model:260]\u001b[0m Enumerating the model containers.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 15:24:34,503 kur.model.model:265]\u001b[0m Assembling the model dependency graph.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:34,503 kur.model.model:272]\u001b[0m Assembled Node: images\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:34,503 kur.model.model:274]\u001b[0m   Uses: \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:34,503 kur.model.model:276]\u001b[0m   Used by: ..convolution.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:34,503 kur.model.model:277]\u001b[0m   Aliases: images\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:34,503 kur.model.model:272]\u001b[0m Assembled Node: ..convolution.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:34,503 kur.model.model:274]\u001b[0m   Uses: images\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:34,503 kur.model.model:276]\u001b[0m   Used by: ..activation.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:34,503 kur.model.model:277]\u001b[0m   Aliases: ..convolution.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:34,504 kur.model.model:272]\u001b[0m Assembled Node: ..activation.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:34,504 kur.model.model:274]\u001b[0m   Uses: ..convolution.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:34,504 kur.model.model:276]\u001b[0m   Used by: ..pool.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:34,504 kur.model.model:277]\u001b[0m   Aliases: ..activation.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:34,504 kur.model.model:272]\u001b[0m Assembled Node: ..pool.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:34,504 kur.model.model:274]\u001b[0m   Uses: ..activation.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:34,504 kur.model.model:276]\u001b[0m   Used by: ..flatten.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:34,504 kur.model.model:277]\u001b[0m   Aliases: ..pool.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:34,504 kur.model.model:272]\u001b[0m Assembled Node: ..flatten.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:34,504 kur.model.model:274]\u001b[0m   Uses: ..pool.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:34,504 kur.model.model:276]\u001b[0m   Used by: ..dense.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:34,504 kur.model.model:277]\u001b[0m   Aliases: ..flatten.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:34,504 kur.model.model:272]\u001b[0m Assembled Node: ..dense.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:34,504 kur.model.model:274]\u001b[0m   Uses: ..flatten.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:34,504 kur.model.model:276]\u001b[0m   Used by: ..dense.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:34,504 kur.model.model:277]\u001b[0m   Aliases: ..dense.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:34,504 kur.model.model:272]\u001b[0m Assembled Node: ..dense.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:34,504 kur.model.model:274]\u001b[0m   Uses: ..dense.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:34,504 kur.model.model:276]\u001b[0m   Used by: labels\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:34,504 kur.model.model:277]\u001b[0m   Aliases: ..dense.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:34,504 kur.model.model:272]\u001b[0m Assembled Node: labels\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:34,505 kur.model.model:274]\u001b[0m   Uses: ..dense.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:34,505 kur.model.model:276]\u001b[0m   Used by: \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:34,505 kur.model.model:277]\u001b[0m   Aliases: labels\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 15:24:34,505 kur.model.model:280]\u001b[0m Connecting the model graph.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:34,505 kur.model.model:311]\u001b[0m Building node: images\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:34,505 kur.model.model:312]\u001b[0m   Aliases: images\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:34,505 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:34,505 kur.containers.layers.placeholder:117]\u001b[0m Creating placeholder for \"images\" with data type \"float32\".\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:34,505 kur.model.model:125]\u001b[0m Trying to infer shape for input \"images\"\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:34,505 kur.model.model:143]\u001b[0m Inferred shape for input \"images\": (32, 32, 3)\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:34,505 kur.containers.layers.placeholder:127]\u001b[0m Inferred shape: (32, 32, 3)\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:34,510 kur.model.model:382]\u001b[0m   Value: images\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:34,510 kur.model.model:311]\u001b[0m Building node: ..convolution.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:34,510 kur.model.model:312]\u001b[0m   Aliases: ..convolution.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:34,510 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:34,510 kur.model.model:315]\u001b[0m   - images: images\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,577 kur.model.model:382]\u001b[0m   Value: Elemwise{add,no_inplace}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,577 kur.model.model:311]\u001b[0m Building node: ..activation.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,577 kur.model.model:312]\u001b[0m   Aliases: ..activation.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,577 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,577 kur.model.model:315]\u001b[0m   - ..convolution.0: Elemwise{add,no_inplace}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,612 kur.model.model:382]\u001b[0m   Value: Elemwise{mul,no_inplace}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,612 kur.model.model:311]\u001b[0m Building node: ..pool.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,612 kur.model.model:312]\u001b[0m   Aliases: ..pool.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,612 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,612 kur.model.model:315]\u001b[0m   - ..activation.0: Elemwise{mul,no_inplace}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,615 kur.model.model:382]\u001b[0m   Value: DimShuffle{0,2,3,1}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,615 kur.model.model:311]\u001b[0m Building node: ..flatten.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,615 kur.model.model:312]\u001b[0m   Aliases: ..flatten.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,615 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,615 kur.model.model:315]\u001b[0m   - ..pool.0: DimShuffle{0,2,3,1}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,622 kur.model.model:382]\u001b[0m   Value: Reshape{2}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,622 kur.model.model:311]\u001b[0m Building node: ..dense.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,622 kur.model.model:312]\u001b[0m   Aliases: ..dense.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,622 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,622 kur.model.model:315]\u001b[0m   - ..flatten.0: Reshape{2}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,624 kur.model.model:382]\u001b[0m   Value: Elemwise{add,no_inplace}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,624 kur.model.model:311]\u001b[0m Building node: ..dense.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,624 kur.model.model:312]\u001b[0m   Aliases: ..dense.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,624 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,624 kur.model.model:315]\u001b[0m   - ..dense.0: Elemwise{add,no_inplace}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,625 kur.model.model:382]\u001b[0m   Value: Elemwise{add,no_inplace}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,625 kur.model.model:311]\u001b[0m Building node: labels\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,625 kur.model.model:312]\u001b[0m   Aliases: labels\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,625 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,625 kur.model.model:315]\u001b[0m   - ..dense.1: Elemwise{add,no_inplace}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,625 kur.model.model:382]\u001b[0m   Value: Softmax.0\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 15:24:35,625 kur.model.model:284]\u001b[0m Model inputs:  images\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 15:24:35,625 kur.model.model:285]\u001b[0m Model outputs: labels\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 15:24:35,625 kur.kurfile:357]\u001b[0m Ignoring missing initial weights: t1_dp0.25/cifar.best.valid.w. If this is undesireable, set \"must_exist\" to \"yes\" in the approriate \"weights\" section.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 15:24:35,625 kur.model.executor:315]\u001b[0m No historical training loss available from logs.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 15:24:35,626 kur.model.executor:323]\u001b[0m No historical validation loss available from logs.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 15:24:35,626 kur.model.executor:329]\u001b[0m No previous epochs.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,626 kur.model.executor:353]\u001b[0m Epoch handling mode: additional\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,626 kur.model.executor:101]\u001b[0m Recompiling the model.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,626 kur.backend.keras_backend:527]\u001b[0m Instantiating a Keras model.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,825 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,825 kur.backend.keras_backend:538]\u001b[0m Layer (type)                     Output Shape          Param #     Connected to                     \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,825 kur.backend.keras_backend:538]\u001b[0m ====================================================================================================\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,825 kur.backend.keras_backend:538]\u001b[0m images (InputLayer)              (None, 32, 32, 3)     0                                            \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,825 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,825 kur.backend.keras_backend:538]\u001b[0m ..convolution.0 (Convolution2D)  (None, 16, 16, 20)    1520        images[0][0]                     \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,825 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,826 kur.backend.keras_backend:538]\u001b[0m ..activation.0 (Activation)      (None, 16, 16, 20)    0           ..convolution.0[0][0]            \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,826 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,826 kur.backend.keras_backend:538]\u001b[0m ..pool.0 (MaxPooling2D)          (None, 8, 8, 20)      0           ..activation.0[0][0]             \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,826 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,826 kur.backend.keras_backend:538]\u001b[0m ..flatten.0 (Flatten)            (None, 1280)          0           ..pool.0[0][0]                   \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,826 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,826 kur.backend.keras_backend:538]\u001b[0m ..dense.0 (Dense)                (None, 15)            19215       ..flatten.0[0][0]                \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,826 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,826 kur.backend.keras_backend:538]\u001b[0m ..dense.1 (Dense)                (None, 10)            160         ..dense.0[0][0]                  \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,826 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,826 kur.backend.keras_backend:538]\u001b[0m labels (Activation)              (None, 10)            0           ..dense.1[0][0]                  \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,826 kur.backend.keras_backend:538]\u001b[0m ====================================================================================================\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,826 kur.backend.keras_backend:538]\u001b[0m Total params: 20,895\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,826 kur.backend.keras_backend:538]\u001b[0m Trainable params: 20,895\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,826 kur.backend.keras_backend:538]\u001b[0m Non-trainable params: 0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,826 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,826 kur.backend.keras_backend:538]\u001b[0m \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,826 kur.backend.keras_backend:576]\u001b[0m Assembling a training function from the model.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:35,831 kur.backend.keras_backend:509]\u001b[0m Adding additional inputs: labels\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:37,417 kur.backend.keras_backend:599]\u001b[0m Additional inputs for log functions: labels\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:37,417 kur.backend.keras_backend:616]\u001b[0m Expected input shapes: images=(None, 32, 32, 3), labels=(None, None)\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:37,417 kur.backend.keras_backend:634]\u001b[0m Compiled model: {'names': {'output': ['labels', 'labels'], 'input': ['images', 'labels']}, 'func': <keras.backend.theano_backend.Function object at 0x129e70ef0>, 'shapes': {'input': [(None, 32, 32, 3), (None, None)]}}\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:37,417 kur.providers.batch_provider:57]\u001b[0m Batch size set to: 2\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:37,417 kur.providers.batch_provider:102]\u001b[0m Maximum number of batches set to: 1\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 15:24:37,420 kur.backend.keras_backend:666]\u001b[0m Waiting for model to finish compiling...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:37,420 kur.providers.batch_provider:139]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:37,420 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:37,469 kur.providers.provider:144]\u001b[0m Data source \"labels\": entries=10000, shape=(10,)\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:37,470 kur.providers.provider:144]\u001b[0m Data source \"images\": entries=10000, shape=(32, 32, 3)\u001b[0m\n",
      "\n",
      "Epoch 1/1, loss=N/A:   0%|                       | 0/10000 [00:00<?, ?samples/s]\u001b[1;34m[DEBUG 2017-03-01 15:24:37,475 kur.providers.shuffle_provider:184]\u001b[0m Shuffling...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:37,646 kur.providers.batch_provider:139]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:37,647 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:37,647 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:37,656 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:37,681 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:37,682 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:37,682 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: batch_loss_total\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:37,683 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: batch_loss_labels\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:37,683 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: batch_loss_batch\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:37,683 kur.loggers.binary_logger:144]\u001b[0m Writing logger summary.\u001b[0m\n",
      "Epoch 1/1, loss=2.321:   1%|▏         | 128/10000 [00:00<00:16, 610.76samples/s]\u001b[1;34m[DEBUG 2017-03-01 15:24:37,685 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:37,694 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:37,698 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:37,717 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:37,717 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:37,718 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:37,730 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:37,754 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:37,755 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:37,755 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:37,772 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:37,797 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.325:   5%|▌         | 512/10000 [00:00<00:11, 810.28samples/s]\u001b[1;34m[DEBUG 2017-03-01 15:24:37,798 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:37,798 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:37,813 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:37,829 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:37,830 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:37,838 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:37,844 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:37,868 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:37,869 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:37,869 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:37,882 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:37,902 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.314:   9%|▊        | 896/10000 [00:00<00:08, 1057.23samples/s]\u001b[1;34m[DEBUG 2017-03-01 15:24:37,902 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:37,903 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:37,916 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:37,938 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:37,940 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:37,951 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:37,957 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:37,977 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:37,977 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:37,977 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:37,990 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,011 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.304:  13%|█       | 1280/10000 [00:00<00:06, 1338.00samples/s]\u001b[1;34m[DEBUG 2017-03-01 15:24:38,012 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,012 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,028 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,046 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,047 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,047 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,063 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,086 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,086 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,086 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,097 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,127 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.287:  17%|█▎      | 1664/10000 [00:00<00:05, 1630.08samples/s]\u001b[1;34m[DEBUG 2017-03-01 15:24:38,127 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,127 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,137 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,164 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,165 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,165 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,178 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,199 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,199 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,200 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,210 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,234 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.277:  20%|█▋      | 2048/10000 [00:00<00:04, 1947.87samples/s]\u001b[1;34m[DEBUG 2017-03-01 15:24:38,235 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,235 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,244 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,270 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,270 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,270 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,278 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,301 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,302 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,302 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,311 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,333 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,334 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,334 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,346 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,370 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.261:  26%|██      | 2560/10000 [00:00<00:03, 2278.82samples/s]\u001b[1;34m[DEBUG 2017-03-01 15:24:38,370 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,371 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,384 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,405 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,405 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,405 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,408 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,435 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,436 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,436 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,446 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,472 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.249:  29%|██▎     | 2944/10000 [00:00<00:02, 2584.62samples/s]\u001b[1;34m[DEBUG 2017-03-01 15:24:38,472 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,479 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,484 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,500 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,501 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,501 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,512 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,530 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,531 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,539 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,543 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,563 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,564 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,564 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,574 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,591 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.228:  35%|██▊     | 3456/10000 [00:01<00:02, 2937.59samples/s]\u001b[1;34m[DEBUG 2017-03-01 15:24:38,591 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,601 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,605 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,626 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,627 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,627 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,638 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,659 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,660 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,660 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,674 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,690 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,691 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,701 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,705 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,723 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.210:  40%|███▏    | 3968/10000 [00:01<00:01, 3167.84samples/s]\u001b[1;34m[DEBUG 2017-03-01 15:24:38,723 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,732 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,736 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,754 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,755 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,765 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,770 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,789 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,790 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,790 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,800 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,817 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,818 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,818 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,831 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,848 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.188:  45%|███▌    | 4480/10000 [00:01<00:01, 3398.18samples/s]\u001b[1;34m[DEBUG 2017-03-01 15:24:38,848 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,856 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,860 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,887 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,888 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,888 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,903 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,922 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,922 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,923 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,936 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,955 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.178:  49%|███▉    | 4864/10000 [00:01<00:01, 3449.12samples/s]\u001b[1;34m[DEBUG 2017-03-01 15:24:38,956 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,956 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,970 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,990 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,990 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:38,998 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,002 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,022 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,023 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,023 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,037 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,057 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.167:  52%|████▏   | 5248/10000 [00:01<00:01, 3540.87samples/s]\u001b[1;34m[DEBUG 2017-03-01 15:24:39,057 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,058 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,069 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,089 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,090 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,090 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,101 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,119 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,119 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,130 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,134 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,155 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,156 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,156 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,170 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,192 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.152:  58%|████▌   | 5760/10000 [00:01<00:01, 3609.14samples/s]\u001b[1;34m[DEBUG 2017-03-01 15:24:39,193 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,203 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,206 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,222 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,223 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,223 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,237 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,256 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,256 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,263 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,267 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,283 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,284 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,284 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,297 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,317 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.135:  63%|█████   | 6272/10000 [00:01<00:00, 3746.78samples/s]\u001b[1;34m[DEBUG 2017-03-01 15:24:39,317 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,325 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,328 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,347 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,348 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,349 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,358 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,384 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,384 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,392 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,396 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,416 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,418 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,418 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,427 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,449 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.119:  68%|█████▍  | 6784/10000 [00:01<00:00, 3780.45samples/s]\u001b[1;34m[DEBUG 2017-03-01 15:24:39,451 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,451 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,469 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,486 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,487 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,487 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,501 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,519 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,519 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,527 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,532 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,551 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,552 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,552 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,566 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,582 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.104:  73%|█████▊  | 7296/10000 [00:02<00:00, 3803.34samples/s]\u001b[1;34m[DEBUG 2017-03-01 15:24:39,583 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,583 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,592 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,615 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,616 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,616 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,624 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,646 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,647 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,647 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,659 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,678 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,679 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,679 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,689 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,712 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.092:  78%|██████▏ | 7808/10000 [00:02<00:00, 3847.20samples/s]\u001b[1;34m[DEBUG 2017-03-01 15:24:39,712 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,712 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,724 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,743 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,743 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,743 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,754 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,777 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,778 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,778 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,779 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,820 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,821 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,821 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,833 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,852 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.078:  83%|██████▋ | 8320/10000 [00:02<00:00, 3789.42samples/s]\u001b[1;34m[DEBUG 2017-03-01 15:24:39,852 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,852 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,864 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,880 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,881 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,881 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,894 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,915 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,916 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,916 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,917 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,949 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,949 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,950 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,965 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,987 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.067:  88%|███████ | 8832/10000 [00:02<00:00, 3785.63samples/s]\u001b[1;34m[DEBUG 2017-03-01 15:24:39,988 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:39,988 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,001 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,022 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,023 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,024 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,037 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,055 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,056 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,067 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,070 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,089 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.058:  92%|███████▎| 9216/10000 [00:02<00:00, 3775.74samples/s]\u001b[1;34m[DEBUG 2017-03-01 15:24:40,090 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,090 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,103 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,121 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,121 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,121 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,134 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,153 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,153 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,154 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,168 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,190 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.052:  96%|███████▋| 9600/10000 [00:02<00:00, 3788.37samples/s]\u001b[1;34m[DEBUG 2017-03-01 15:24:40,191 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,191 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,205 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,227 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,227 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,238 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,242 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,263 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,264 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,264 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,265 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,300 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.043: 100%|███████▉| 9984/10000 [00:02<00:00, 3695.83samples/s]\u001b[1;34m[DEBUG 2017-03-01 15:24:40,301 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,309 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.042: 100%|███████| 10000/10000 [00:02<00:00, 3528.21samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-01 15:24:40,310 kur.model.executor:464]\u001b[0m Training loss: 2.042\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 15:24:40,310 kur.model.executor:471]\u001b[0m Saving best historical training weights: t1_dp0.25/cifar.best.train.w\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,310 kur.model.executor:430]\u001b[0m Saving weights to: t1_dp0.25/cifar.best.train.w\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,310 kur.model.model:213]\u001b[0m Saving model weights to: t1_dp0.25/cifar.best.train.w\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,316 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: training_loss_total\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,317 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: training_loss_labels\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,318 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: training_loss_batch\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,319 kur.loggers.binary_logger:144]\u001b[0m Writing logger summary.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,320 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: batch_loss_total\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,321 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: batch_loss_labels\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,322 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: batch_loss_batch\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,323 kur.loggers.binary_logger:144]\u001b[0m Writing logger summary.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,325 kur.model.executor:101]\u001b[0m Recompiling the model.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,327 kur.backend.keras_backend:543]\u001b[0m Reusing an existing model.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,327 kur.backend.keras_backend:560]\u001b[0m Assembling a testing function from the model.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,333 kur.backend.keras_backend:509]\u001b[0m Adding additional inputs: labels\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,707 kur.backend.keras_backend:599]\u001b[0m Additional inputs for log functions: labels\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,707 kur.backend.keras_backend:616]\u001b[0m Expected input shapes: images=(None, 32, 32, 3), labels=(None, None)\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,707 kur.backend.keras_backend:634]\u001b[0m Compiled model: {'names': {'output': ['labels', 'labels'], 'input': ['images', 'labels']}, 'func': <keras.backend.theano_backend.Function object at 0x10c5e4320>, 'shapes': {'input': [(None, 32, 32, 3), (None, None)]}}\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,707 kur.providers.batch_provider:57]\u001b[0m Batch size set to: 2\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,708 kur.providers.batch_provider:102]\u001b[0m Maximum number of batches set to: 1\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 15:24:40,713 kur.backend.keras_backend:666]\u001b[0m Waiting for model to finish compiling...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,713 kur.providers.batch_provider:139]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,713 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "Validating, loss=N/A:   0%|                       | 0/6400 [00:00<?, ?samples/s]\u001b[1;34m[DEBUG 2017-03-01 15:24:40,717 kur.providers.shuffle_provider:184]\u001b[0m Shuffling...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,853 kur.providers.batch_provider:139]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,854 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,855 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,871 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "Validating, loss=1.848:   2%|▏         | 128/6400 [00:00<00:07, 821.70samples/s]\u001b[1;34m[DEBUG 2017-03-01 15:24:40,873 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,874 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,887 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,900 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,901 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,909 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,914 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,922 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,925 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,933 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,943 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,947 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,948 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,959 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,966 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,969 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,970 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,981 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "Validating, loss=1.787:  20%|█▌      | 1280/6400 [00:00<00:04, 1135.88samples/s]\u001b[1;34m[DEBUG 2017-03-01 15:24:40,983 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,992 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:40,997 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,005 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,009 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,021 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,022 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,030 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,035 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,045 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,055 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,060 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,070 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,075 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,075 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,087 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "Validating, loss=1.799:  36%|██▉     | 2304/6400 [00:00<00:02, 1544.41samples/s]\u001b[1;34m[DEBUG 2017-03-01 15:24:41,097 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,102 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,102 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,114 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,119 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,129 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,132 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,145 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,147 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,160 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,160 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,172 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,182 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,188 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,189 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,202 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "Validating, loss=1.809:  52%|████▏   | 3328/6400 [00:00<00:01, 2055.80samples/s]\u001b[1;34m[DEBUG 2017-03-01 15:24:41,203 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,213 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,228 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,233 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,244 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,249 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,250 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,256 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,261 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,275 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,283 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,287 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,288 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,297 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,302 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,311 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "Validating, loss=1.811:  68%|█████▍  | 4352/6400 [00:00<00:00, 2677.92samples/s]\u001b[1;34m[DEBUG 2017-03-01 15:24:41,323 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,328 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,328 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,337 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,342 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,348 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,353 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,362 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,364 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,373 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,378 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,389 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,391 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,399 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,402 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,412 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "Validating, loss=1.814:  84%|██████▋ | 5376/6400 [00:00<00:00, 3434.82samples/s]\u001b[1;34m[DEBUG 2017-03-01 15:24:41,416 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,426 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,431 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,440 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,443 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,453 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,458 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,465 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,469 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,484 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,485 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,492 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,497 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,504 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "Validating, loss=1.815: 100%|████████| 6400/6400 [00:00<00:00, 4241.72samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-01 15:24:41,525 kur.model.executor:197]\u001b[0m Validation loss: 1.815\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-01 15:24:41,526 kur.model.executor:413]\u001b[0m Saving best historical validation weights: t1_dp0.25/cifar.best.valid.w\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,526 kur.model.executor:444]\u001b[0m Copying weights from: t1_dp0.25/cifar.best.train.w\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,529 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: validation_loss_total\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,530 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: validation_loss_labels\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,531 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: validation_loss_batch\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,533 kur.loggers.binary_logger:144]\u001b[0m Writing logger summary.\u001b[0m\n",
      "Completed 1 epochs.\n",
      "\u001b[1;37m[INFO 2017-03-01 15:24:41,537 kur.model.executor:235]\u001b[0m Saving most recent weights: t1_dp0.25/cifar.last.w\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-01 15:24:41,537 kur.model.model:213]\u001b[0m Saving model weights to: t1_dp0.25/cifar.last.w\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!kur -vv train dlnd_p2_fluid.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Simplify Kurfile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dlnd_p2_dropout.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile dlnd_p2_dropout.yml\n",
    "\n",
    "---\n",
    "settings:\n",
    "\n",
    "  # Where to get the data\n",
    "  cifar: &cifar\n",
    "    url: \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "    checksum: \"6d958be074577803d12ecdefd02955f39262c83c16fe9348329d7fe0b5c001ce\"\n",
    "    path: \"~/kur\"           # if kur does not have normalization or one-hot-encoding, ##################\n",
    "                            # without normalization and one-hot-encoding, the performance will be hurt, right? ####\n",
    "  # Backend to use                                      \n",
    "  backend:\n",
    "    name: keras\n",
    "\n",
    "  # Hyperparameters\n",
    "#   cnn:\n",
    "#     kernels: 20\n",
    "#     size: [5, 5]\n",
    "#     strides: [2, 2]                \n",
    "\n",
    "#   pool:\n",
    "#     size: [2,2]\n",
    "#     strides: [2,2]\n",
    "#     type: max                         \n",
    "\n",
    "model:\n",
    "  - input: images                       \n",
    "  - convolution:                       \n",
    "      kernels: 20\n",
    "      size: [5, 5]\n",
    "      strides: [2,2]\n",
    "  - activation: relu\n",
    "  - pool:\n",
    "      size: [2,2]\n",
    "      strides: [2,2]\n",
    "      type: max\n",
    "  - flatten:\n",
    "  - dense: 15                           \n",
    "  - dropout: 0.25\n",
    "  - dense: 10\n",
    "  - activation: softmax\n",
    "    name: labels\n",
    "\n",
    "train:\n",
    "  data:\n",
    "    - cifar:\n",
    "        <<: *cifar\n",
    "        parts: [1]                      \n",
    "  provider: &provider\n",
    "    batch_size: 128\n",
    "    num_batches: 3                        # use all batches\n",
    "    randomize: True\n",
    "  log: t1_dp0.25/cifar-log                   \n",
    "  epochs: 1\n",
    "  weights:\n",
    "    initial: t1_dp0.25/cifar.best.valid.w    \n",
    "    best: t1_dp0.25/cifar.best.train.w\n",
    "    last: t1_dp0.25/cifar.last.w\n",
    "\n",
    "  optimizer:\n",
    "    name: adam\n",
    "    learning_rate: 0.001\n",
    "\n",
    "validate:\n",
    "  data:\n",
    "    - cifar:\n",
    "       <<: *cifar\n",
    "       parts: 5                          \n",
    "  provider: \n",
    "    <<: *provider       # can I do this? ##########\n",
    "    num_batches: 2     # override num_batches to be 50 while keep batch_size and randomize as 128 and True ####\n",
    "  weights: t1_dp0.25/cifar.best.valid.w\n",
    "\n",
    "test: &test\n",
    "  data:\n",
    "    - cifar:\n",
    "       <<: *cifar\n",
    "       parts: test\n",
    "  weights: t1_dp0.25/cifar.best.valid.w\n",
    "  provider: \n",
    "    <<: *provider\n",
    "    randomize: False\n",
    "\n",
    "\n",
    "evaluate:\n",
    "  <<: *test\n",
    "  destination: t1_dp0.25/cifar.results.pkl\n",
    "\n",
    "loss:\n",
    "  - target: labels                        # in the project: training loss and valid_accuracy are printed #############\n",
    "    name: categorical_crossentropy        # this should be a matter of personal taste, won't really affect anything##\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;37m[INFO 2017-03-02 20:31:03,897 kur.kurfile:699]\u001b[0m Parsing source: dlnd_p2_dropout.yml, included by top-level.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-02 20:31:03,910 kur.kurfile:82]\u001b[0m Parsing Kurfile...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:03,910 kur.kurfile:784]\u001b[0m Parsing Kurfile section: settings\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:03,914 kur.kurfile:784]\u001b[0m Parsing Kurfile section: train\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:03,917 kur.kurfile:784]\u001b[0m Parsing Kurfile section: validate\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:03,919 kur.kurfile:784]\u001b[0m Parsing Kurfile section: test\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:03,921 kur.kurfile:784]\u001b[0m Parsing Kurfile section: evaluate\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:03,925 kur.containers.layers.placeholder:63]\u001b[0m Using short-hand name for placeholder: images\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:03,925 kur.containers.layers.placeholder:97]\u001b[0m Placeholder \"images\" has a deferred shape.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:03,927 kur.kurfile:784]\u001b[0m Parsing Kurfile section: loss\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-02 20:31:03,929 kur.loggers.binary_logger:107]\u001b[0m Log does not exist. Creating path: t1_dp0.25/cifar-log\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:04,727 kur.utils.package:233]\u001b[0m File exists and passed checksum: /Users/Natsume/kur/cifar-10-python.tar.gz\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:11,034 kur.providers.batch_provider:57]\u001b[0m Batch size set to: 128\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:11,034 kur.providers.batch_provider:102]\u001b[0m Maximum number of batches set to: 3\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:11,766 kur.utils.package:233]\u001b[0m File exists and passed checksum: /Users/Natsume/kur/cifar-10-python.tar.gz\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:17,439 kur.providers.batch_provider:57]\u001b[0m Batch size set to: 128\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:17,439 kur.providers.batch_provider:102]\u001b[0m Maximum number of batches set to: 2\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:17,439 kur.backend.backend:187]\u001b[0m Using backend: keras\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-02 20:31:17,440 kur.backend.backend:80]\u001b[0m Creating backend: keras\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-02 20:31:17,440 kur.backend.backend:83]\u001b[0m Backend variants: none\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-02 20:31:17,440 kur.backend.keras_backend:122]\u001b[0m No particular backend for Keras has been requested.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:17,440 kur.backend.keras_backend:124]\u001b[0m Using the system-default Keras backend.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:17,440 kur.backend.keras_backend:189]\u001b[0m Overriding environmental variables: {'THEANO_FLAGS': None, 'TF_CPP_MIN_LOG_LEVEL': '1', 'KERAS_BACKEND': None}\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-02 20:31:18,398 kur.backend.keras_backend:195]\u001b[0m Keras is loaded. The backend is: theano\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-02 20:31:18,399 kur.model.model:260]\u001b[0m Enumerating the model containers.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-02 20:31:18,399 kur.model.model:265]\u001b[0m Assembling the model dependency graph.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:18,400 kur.model.model:272]\u001b[0m Assembled Node: images\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:18,400 kur.model.model:274]\u001b[0m   Uses: \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:18,400 kur.model.model:276]\u001b[0m   Used by: ..convolution.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:18,400 kur.model.model:277]\u001b[0m   Aliases: images\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:18,400 kur.model.model:272]\u001b[0m Assembled Node: ..convolution.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:18,400 kur.model.model:274]\u001b[0m   Uses: images\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:18,400 kur.model.model:276]\u001b[0m   Used by: ..activation.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:18,400 kur.model.model:277]\u001b[0m   Aliases: ..convolution.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:18,400 kur.model.model:272]\u001b[0m Assembled Node: ..activation.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:18,400 kur.model.model:274]\u001b[0m   Uses: ..convolution.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:18,400 kur.model.model:276]\u001b[0m   Used by: ..pool.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:18,401 kur.model.model:277]\u001b[0m   Aliases: ..activation.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:18,401 kur.model.model:272]\u001b[0m Assembled Node: ..pool.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:18,401 kur.model.model:274]\u001b[0m   Uses: ..activation.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:18,401 kur.model.model:276]\u001b[0m   Used by: ..flatten.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:18,401 kur.model.model:277]\u001b[0m   Aliases: ..pool.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:18,401 kur.model.model:272]\u001b[0m Assembled Node: ..flatten.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:18,401 kur.model.model:274]\u001b[0m   Uses: ..pool.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:18,401 kur.model.model:276]\u001b[0m   Used by: ..dense.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:18,401 kur.model.model:277]\u001b[0m   Aliases: ..flatten.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:18,401 kur.model.model:272]\u001b[0m Assembled Node: ..dense.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:18,401 kur.model.model:274]\u001b[0m   Uses: ..flatten.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:18,401 kur.model.model:276]\u001b[0m   Used by: ..dropout.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:18,401 kur.model.model:277]\u001b[0m   Aliases: ..dense.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:18,401 kur.model.model:272]\u001b[0m Assembled Node: ..dropout.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:18,401 kur.model.model:274]\u001b[0m   Uses: ..dense.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:18,401 kur.model.model:276]\u001b[0m   Used by: ..dense.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:18,401 kur.model.model:277]\u001b[0m   Aliases: ..dropout.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:18,401 kur.model.model:272]\u001b[0m Assembled Node: ..dense.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:18,401 kur.model.model:274]\u001b[0m   Uses: ..dropout.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:18,401 kur.model.model:276]\u001b[0m   Used by: labels\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:18,402 kur.model.model:277]\u001b[0m   Aliases: ..dense.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:18,402 kur.model.model:272]\u001b[0m Assembled Node: labels\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:18,402 kur.model.model:274]\u001b[0m   Uses: ..dense.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:18,402 kur.model.model:276]\u001b[0m   Used by: \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:18,402 kur.model.model:277]\u001b[0m   Aliases: labels\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-02 20:31:18,402 kur.model.model:280]\u001b[0m Connecting the model graph.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:18,402 kur.model.model:311]\u001b[0m Building node: images\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:18,402 kur.model.model:312]\u001b[0m   Aliases: images\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:18,402 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:18,402 kur.containers.layers.placeholder:117]\u001b[0m Creating placeholder for \"images\" with data type \"float32\".\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:18,402 kur.model.model:125]\u001b[0m Trying to infer shape for input \"images\"\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:18,402 kur.model.model:143]\u001b[0m Inferred shape for input \"images\": (32, 32, 3)\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:18,402 kur.containers.layers.placeholder:127]\u001b[0m Inferred shape: (32, 32, 3)\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:18,408 kur.model.model:382]\u001b[0m   Value: images\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:18,408 kur.model.model:311]\u001b[0m Building node: ..convolution.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:18,408 kur.model.model:312]\u001b[0m   Aliases: ..convolution.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:18,408 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:18,408 kur.model.model:315]\u001b[0m   - images: images\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:19,584 kur.model.model:382]\u001b[0m   Value: Elemwise{add,no_inplace}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:19,585 kur.model.model:311]\u001b[0m Building node: ..activation.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:19,585 kur.model.model:312]\u001b[0m   Aliases: ..activation.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:19,585 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:19,585 kur.model.model:315]\u001b[0m   - ..convolution.0: Elemwise{add,no_inplace}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:19,623 kur.model.model:382]\u001b[0m   Value: Elemwise{mul,no_inplace}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:19,623 kur.model.model:311]\u001b[0m Building node: ..pool.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:19,623 kur.model.model:312]\u001b[0m   Aliases: ..pool.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:19,623 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:19,623 kur.model.model:315]\u001b[0m   - ..activation.0: Elemwise{mul,no_inplace}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:19,626 kur.model.model:382]\u001b[0m   Value: DimShuffle{0,2,3,1}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:19,626 kur.model.model:311]\u001b[0m Building node: ..flatten.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:19,626 kur.model.model:312]\u001b[0m   Aliases: ..flatten.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:19,627 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:19,627 kur.model.model:315]\u001b[0m   - ..pool.0: DimShuffle{0,2,3,1}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:19,636 kur.model.model:382]\u001b[0m   Value: Reshape{2}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:19,636 kur.model.model:311]\u001b[0m Building node: ..dense.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:19,636 kur.model.model:312]\u001b[0m   Aliases: ..dense.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:19,636 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:19,636 kur.model.model:315]\u001b[0m   - ..flatten.0: Reshape{2}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:19,638 kur.model.model:382]\u001b[0m   Value: Elemwise{add,no_inplace}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:19,638 kur.model.model:311]\u001b[0m Building node: ..dropout.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:19,639 kur.model.model:312]\u001b[0m   Aliases: ..dropout.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:19,639 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:19,639 kur.model.model:315]\u001b[0m   - ..dense.0: Elemwise{add,no_inplace}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:19,752 kur.model.model:382]\u001b[0m   Value: if{}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:19,752 kur.model.model:311]\u001b[0m Building node: ..dense.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:19,752 kur.model.model:312]\u001b[0m   Aliases: ..dense.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:19,752 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:19,752 kur.model.model:315]\u001b[0m   - ..dropout.0: if{}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:19,753 kur.model.model:382]\u001b[0m   Value: Elemwise{add,no_inplace}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:19,753 kur.model.model:311]\u001b[0m Building node: labels\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:19,754 kur.model.model:312]\u001b[0m   Aliases: labels\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:19,754 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:19,754 kur.model.model:315]\u001b[0m   - ..dense.1: Elemwise{add,no_inplace}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:19,754 kur.model.model:382]\u001b[0m   Value: Softmax.0\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-02 20:31:19,754 kur.model.model:284]\u001b[0m Model inputs:  images\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-02 20:31:19,754 kur.model.model:285]\u001b[0m Model outputs: labels\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-02 20:31:19,754 kur.kurfile:357]\u001b[0m Ignoring missing initial weights: t1_dp0.25/cifar.best.valid.w. If this is undesireable, set \"must_exist\" to \"yes\" in the approriate \"weights\" section.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-02 20:31:19,754 kur.model.executor:315]\u001b[0m No historical training loss available from logs.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-02 20:31:19,755 kur.model.executor:323]\u001b[0m No historical validation loss available from logs.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-02 20:31:19,755 kur.model.executor:329]\u001b[0m No previous epochs.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:19,755 kur.model.executor:353]\u001b[0m Epoch handling mode: additional\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:19,755 kur.model.executor:101]\u001b[0m Recompiling the model.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:19,755 kur.backend.keras_backend:527]\u001b[0m Instantiating a Keras model.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:20,003 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:20,003 kur.backend.keras_backend:538]\u001b[0m Layer (type)                     Output Shape          Param #     Connected to                     \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:20,003 kur.backend.keras_backend:538]\u001b[0m ====================================================================================================\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:20,003 kur.backend.keras_backend:538]\u001b[0m images (InputLayer)              (None, 32, 32, 3)     0                                            \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:20,003 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:20,003 kur.backend.keras_backend:538]\u001b[0m ..convolution.0 (Convolution2D)  (None, 16, 16, 20)    1520        images[0][0]                     \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:20,003 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:20,003 kur.backend.keras_backend:538]\u001b[0m ..activation.0 (Activation)      (None, 16, 16, 20)    0           ..convolution.0[0][0]            \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:20,003 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:20,003 kur.backend.keras_backend:538]\u001b[0m ..pool.0 (MaxPooling2D)          (None, 8, 8, 20)      0           ..activation.0[0][0]             \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:20,003 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:20,003 kur.backend.keras_backend:538]\u001b[0m ..flatten.0 (Flatten)            (None, 1280)          0           ..pool.0[0][0]                   \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:20,003 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:20,003 kur.backend.keras_backend:538]\u001b[0m ..dense.0 (Dense)                (None, 15)            19215       ..flatten.0[0][0]                \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:20,003 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:20,003 kur.backend.keras_backend:538]\u001b[0m ..dropout.0 (Dropout)            (None, 15)            0           ..dense.0[0][0]                  \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:20,003 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:20,003 kur.backend.keras_backend:538]\u001b[0m ..dense.1 (Dense)                (None, 10)            160         ..dropout.0[0][0]                \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:20,003 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:20,004 kur.backend.keras_backend:538]\u001b[0m labels (Activation)              (None, 10)            0           ..dense.1[0][0]                  \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:20,004 kur.backend.keras_backend:538]\u001b[0m ====================================================================================================\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:20,004 kur.backend.keras_backend:538]\u001b[0m Total params: 20,895\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:20,004 kur.backend.keras_backend:538]\u001b[0m Trainable params: 20,895\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:20,004 kur.backend.keras_backend:538]\u001b[0m Non-trainable params: 0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:20,004 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:20,004 kur.backend.keras_backend:538]\u001b[0m \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:20,004 kur.backend.keras_backend:576]\u001b[0m Assembling a training function from the model.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:20,011 kur.backend.keras_backend:509]\u001b[0m Adding additional inputs: labels\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:21,492 kur.backend.keras_backend:599]\u001b[0m Additional inputs for log functions: labels\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:21,492 kur.backend.keras_backend:616]\u001b[0m Expected input shapes: images=(None, 32, 32, 3), labels=(None, None)\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:21,492 kur.backend.keras_backend:634]\u001b[0m Compiled model: {'names': {'output': ['labels', 'labels'], 'input': ['images', 'labels']}, 'shapes': {'input': [(None, 32, 32, 3), (None, None)]}, 'func': <keras.backend.theano_backend.Function object at 0x11f7def28>}\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:21,493 kur.providers.batch_provider:57]\u001b[0m Batch size set to: 2\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:21,493 kur.providers.batch_provider:102]\u001b[0m Maximum number of batches set to: 1\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-02 20:31:21,497 kur.backend.keras_backend:666]\u001b[0m Waiting for model to finish compiling...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:21,497 kur.providers.batch_provider:139]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:21,497 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:21,551 kur.providers.provider:144]\u001b[0m Data source \"images\": entries=10000, shape=(32, 32, 3)\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:21,551 kur.providers.provider:144]\u001b[0m Data source \"labels\": entries=10000, shape=(10,)\u001b[0m\n",
      "\n",
      "Epoch 1/1, loss=N/A:   0%|                         | 0/384 [00:00<?, ?samples/s]\u001b[1;34m[DEBUG 2017-03-02 20:31:21,565 kur.providers.shuffle_provider:184]\u001b[0m Shuffling...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:21,855 kur.providers.batch_provider:139]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:21,856 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:21,856 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:21,865 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:21,868 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:21,894 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:21,894 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: batch_loss_batch\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:21,895 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: batch_loss_total\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:21,895 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: batch_loss_labels\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:21,896 kur.loggers.binary_logger:144]\u001b[0m Writing logger summary.\u001b[0m\n",
      "Epoch 1/1, loss=2.316:  33%|████        | 128/384 [00:00<00:00, 384.88samples/s]\u001b[1;34m[DEBUG 2017-03-02 20:31:21,898 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:21,898 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:21,911 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:21,936 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:21,937 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:21,964 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=2.300: 100%|████████████| 384/384 [00:00<00:00, 962.01samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-02 20:31:21,964 kur.model.executor:464]\u001b[0m Training loss: 2.300\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-02 20:31:21,965 kur.model.executor:471]\u001b[0m Saving best historical training weights: t1_dp0.25/cifar.best.train.w\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:21,965 kur.model.executor:430]\u001b[0m Saving weights to: t1_dp0.25/cifar.best.train.w\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:21,965 kur.model.model:213]\u001b[0m Saving model weights to: t1_dp0.25/cifar.best.train.w\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:21,969 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: batch_loss_batch\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:21,970 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: batch_loss_total\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:21,970 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: batch_loss_labels\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:21,970 kur.loggers.binary_logger:144]\u001b[0m Writing logger summary.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:21,971 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: training_loss_batch\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:21,972 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: training_loss_total\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:21,973 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: training_loss_labels\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:21,973 kur.loggers.binary_logger:144]\u001b[0m Writing logger summary.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:21,974 kur.model.executor:101]\u001b[0m Recompiling the model.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:21,975 kur.backend.keras_backend:543]\u001b[0m Reusing an existing model.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:21,975 kur.backend.keras_backend:560]\u001b[0m Assembling a testing function from the model.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:21,982 kur.backend.keras_backend:509]\u001b[0m Adding additional inputs: labels\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:22,340 kur.backend.keras_backend:599]\u001b[0m Additional inputs for log functions: labels\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:22,340 kur.backend.keras_backend:616]\u001b[0m Expected input shapes: images=(None, 32, 32, 3), labels=(None, None)\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:22,340 kur.backend.keras_backend:634]\u001b[0m Compiled model: {'names': {'output': ['labels', 'labels'], 'input': ['images', 'labels']}, 'shapes': {'input': [(None, 32, 32, 3), (None, None)]}, 'func': <keras.backend.theano_backend.Function object at 0x11fc74400>}\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:22,340 kur.providers.batch_provider:57]\u001b[0m Batch size set to: 2\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:22,341 kur.providers.batch_provider:102]\u001b[0m Maximum number of batches set to: 1\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-02 20:31:22,343 kur.backend.keras_backend:666]\u001b[0m Waiting for model to finish compiling...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:22,343 kur.providers.batch_provider:139]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:22,343 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "Validating, loss=N/A:   0%|                        | 0/256 [00:00<?, ?samples/s]\u001b[1;34m[DEBUG 2017-03-02 20:31:22,346 kur.providers.shuffle_provider:184]\u001b[0m Shuffling...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:22,612 kur.providers.batch_provider:139]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:22,613 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:22,619 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "Validating, loss=2.300:  50%|█████▌     | 128/256 [00:00<00:00, 463.55samples/s]\u001b[1;34m[DEBUG 2017-03-02 20:31:22,622 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "Validating, loss=2.290: 100%|███████████| 256/256 [00:00<00:00, 900.46samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-02 20:31:22,631 kur.model.executor:197]\u001b[0m Validation loss: 2.290\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-02 20:31:22,631 kur.model.executor:413]\u001b[0m Saving best historical validation weights: t1_dp0.25/cifar.best.valid.w\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:22,631 kur.model.executor:444]\u001b[0m Copying weights from: t1_dp0.25/cifar.best.train.w\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:22,633 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: validation_loss_batch\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:22,633 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: validation_loss_total\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:22,634 kur.loggers.binary_logger:135]\u001b[0m Adding data to binary column: validation_loss_labels\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:22,634 kur.loggers.binary_logger:144]\u001b[0m Writing logger summary.\u001b[0m\n",
      "Completed 1 epochs.\n",
      "\u001b[1;37m[INFO 2017-03-02 20:31:22,635 kur.model.executor:235]\u001b[0m Saving most recent weights: t1_dp0.25/cifar.last.w\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-02 20:31:22,636 kur.model.model:213]\u001b[0m Saving model weights to: t1_dp0.25/cifar.last.w\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!kur -vv train dlnd_p2_dropout.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dlnd_p2_defaults_sim.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile dlnd_p2_defaults_sim.yml\n",
    "\n",
    "---\n",
    "settings:\n",
    "\n",
    "  # Where to get the data\n",
    "  cifar: &cifar\n",
    "    url: \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "    checksum: \"6d958be074577803d12ecdefd02955f39262c83c16fe9348329d7fe0b5c001ce\"\n",
    "    path: \"~/kur\"           \n",
    "                            \n",
    "  # Backend to use                                      \n",
    "  backend:\n",
    "    name: keras\n",
    "\n",
    "  # Hyperparameters         # keep parameters inside model and train sections\n",
    "#   cnn:\n",
    "#     kernels: 20\n",
    "#     size: [5, 5]\n",
    "#     strides: [2, 2]                \n",
    "\n",
    "#   pool:\n",
    "#     size: [2,2]\n",
    "#     strides: [2,2]\n",
    "#     type: max                         \n",
    "\n",
    "\n",
    "test: &test\n",
    "  data:\n",
    "    - cifar:\n",
    "       <<: *cifar\n",
    "       parts: test\n",
    "  weights: t1_dp0.25/cifar.best.valid.w\n",
    "  provider: \n",
    "    <<: *provider\n",
    "    randomize: False\n",
    "\n",
    "\n",
    "evaluate:\n",
    "  <<: *test\n",
    "  destination: t1_dp0.25/cifar.results.pkl\n",
    "\n",
    "loss:\n",
    "  - target: labels                       \n",
    "    name: categorical_crossentropy       \n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dlnd_p2_fluid_sim.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile dlnd_p2_fluid_sim.yml\n",
    "\n",
    "---\n",
    "model:\n",
    "  - input: images                       \n",
    "  - convolution:                       \n",
    "      kernels: 20\n",
    "      size: [5, 5]\n",
    "      strides: [2,2]\n",
    "  - activation: relu\n",
    "  - pool:\n",
    "      size: [2,2]\n",
    "      strides: [2,2]\n",
    "      type: max\n",
    "  - flatten:\n",
    "  - dense: 15                           \n",
    "  - dropout: 0.25\n",
    "  - dense: 10\n",
    "  - activation: softmax\n",
    "    name: labels\n",
    "\n",
    "train:\n",
    "  data:\n",
    "    - cifar:\n",
    "        <<: *cifar\n",
    "        parts: [1]                      \n",
    "  provider: &provider\n",
    "    batch_size: 128\n",
    "    num_batches: 1000                        # use all batches\n",
    "    randomize: True\n",
    "  log: t1_dp0.25/cifar-log                   \n",
    "  epochs: 20\n",
    "  weights:\n",
    "    initial: t1_dp0.25/cifar.best.valid.w    \n",
    "    best: t1_dp0.25/cifar.best.train.w\n",
    "    last: t1_dp0.25/cifar.last.w\n",
    "\n",
    "  optimizer:\n",
    "    name: adam\n",
    "    learning_rate: 0.001\n",
    "\n",
    "validate:\n",
    "  data:\n",
    "    - cifar:\n",
    "       <<: *cifar\n",
    "       parts: 5                          \n",
    "  provider: \n",
    "    <<: *provider       # can I do this? ##########\n",
    "    num_batches: 50     # override num_batches to be 50 while keep batch_size and randomize as 128 and True ####\n",
    "  weights: t1_dp0.25/cifar.best.valid.w\n",
    "\n",
    "include: dlnd_p2_defaults_sim.yml  # to define default setting\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;37m[INFO 2017-03-02 20:47:17,544 kur.kurfile:699]\u001b[0m Parsing source: dlnd_p2_fluid_sim.yml, included by top-level.\u001b[0m\n",
      "\u001b[1;31m[ERROR 2017-03-02 20:47:17,549 kur.kurfile:723]\u001b[0m Failed to read file: dlnd_p2_fluid_sim.yml. Check your syntax.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Natsume/Downloads/kur_road/kur/kur/kurfile.py\", line 720, in parse_source\n",
      "    data = Reader.read_file(expanded)\n",
      "  File \"/Users/Natsume/Downloads/kur_road/kur/kur/reader/reader.py\", line 109, in read_file\n",
      "    return reader.read(fh.read())\n",
      "  File \"/Users/Natsume/Downloads/kur_road/kur/kur/reader/yaml_reader.py\", line 56, in read\n",
      "    return yaml.load(data)\n",
      "  File \"/Users/Natsume/miniconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/yaml/__init__.py\", line 72, in load\n",
      "    return loader.get_single_data()\n",
      "  File \"/Users/Natsume/miniconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/yaml/constructor.py\", line 35, in get_single_data\n",
      "    node = self.get_single_node()\n",
      "  File \"/Users/Natsume/miniconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/yaml/composer.py\", line 36, in get_single_node\n",
      "    document = self.compose_document()\n",
      "  File \"/Users/Natsume/miniconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/yaml/composer.py\", line 55, in compose_document\n",
      "    node = self.compose_node(None, None)\n",
      "  File \"/Users/Natsume/miniconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/yaml/composer.py\", line 84, in compose_node\n",
      "    node = self.compose_mapping_node(anchor)\n",
      "  File \"/Users/Natsume/miniconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/yaml/composer.py\", line 133, in compose_mapping_node\n",
      "    item_value = self.compose_node(node, item_key)\n",
      "  File \"/Users/Natsume/miniconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/yaml/composer.py\", line 84, in compose_node\n",
      "    node = self.compose_mapping_node(anchor)\n",
      "  File \"/Users/Natsume/miniconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/yaml/composer.py\", line 133, in compose_mapping_node\n",
      "    item_value = self.compose_node(node, item_key)\n",
      "  File \"/Users/Natsume/miniconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/yaml/composer.py\", line 82, in compose_node\n",
      "    node = self.compose_sequence_node(anchor)\n",
      "  File \"/Users/Natsume/miniconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/yaml/composer.py\", line 111, in compose_sequence_node\n",
      "    node.value.append(self.compose_node(node, index))\n",
      "  File \"/Users/Natsume/miniconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/yaml/composer.py\", line 84, in compose_node\n",
      "    node = self.compose_mapping_node(anchor)\n",
      "  File \"/Users/Natsume/miniconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/yaml/composer.py\", line 133, in compose_mapping_node\n",
      "    item_value = self.compose_node(node, item_key)\n",
      "  File \"/Users/Natsume/miniconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/yaml/composer.py\", line 84, in compose_node\n",
      "    node = self.compose_mapping_node(anchor)\n",
      "  File \"/Users/Natsume/miniconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/yaml/composer.py\", line 133, in compose_mapping_node\n",
      "    item_value = self.compose_node(node, item_key)\n",
      "  File \"/Users/Natsume/miniconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/yaml/composer.py\", line 69, in compose_node\n",
      "    % anchor, event.start_mark)\n",
      "yaml.composer.ComposerError: found undefined alias 'cifar'\n",
      "  in \"<unicode string>\", line 24, column 13:\n",
      "            <<: *cifar\n",
      "                ^\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Natsume/miniconda2/envs/dlnd-tf-lab/bin/kur\", line 11, in <module>\n",
      "    load_entry_point('kur', 'console_scripts', 'kur')()\n",
      "  File \"/Users/Natsume/Downloads/kur_road/kur/kur/__main__.py\", line 382, in main\n",
      "    sys.exit(args.func(args) or 0)\n",
      "  File \"/Users/Natsume/Downloads/kur_road/kur/kur/__main__.py\", line 60, in train\n",
      "    spec = parse_kurfile(args.kurfile, args.engine)\n",
      "  File \"/Users/Natsume/Downloads/kur_road/kur/kur/__main__.py\", line 45, in parse_kurfile\n",
      "    spec = Kurfile(filename, engine)\n",
      "  File \"/Users/Natsume/Downloads/kur_road/kur/kur/kurfile.py\", line 66, in __init__\n",
      "    context=None\n",
      "  File \"/Users/Natsume/Downloads/kur_road/kur/kur/kurfile.py\", line 720, in parse_source\n",
      "    data = Reader.read_file(expanded)\n",
      "  File \"/Users/Natsume/Downloads/kur_road/kur/kur/reader/reader.py\", line 109, in read_file\n",
      "    return reader.read(fh.read())\n",
      "  File \"/Users/Natsume/Downloads/kur_road/kur/kur/reader/yaml_reader.py\", line 56, in read\n",
      "    return yaml.load(data)\n",
      "  File \"/Users/Natsume/miniconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/yaml/__init__.py\", line 72, in load\n",
      "    return loader.get_single_data()\n",
      "  File \"/Users/Natsume/miniconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/yaml/constructor.py\", line 35, in get_single_data\n",
      "    node = self.get_single_node()\n",
      "  File \"/Users/Natsume/miniconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/yaml/composer.py\", line 36, in get_single_node\n",
      "    document = self.compose_document()\n",
      "  File \"/Users/Natsume/miniconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/yaml/composer.py\", line 55, in compose_document\n",
      "    node = self.compose_node(None, None)\n",
      "  File \"/Users/Natsume/miniconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/yaml/composer.py\", line 84, in compose_node\n",
      "    node = self.compose_mapping_node(anchor)\n",
      "  File \"/Users/Natsume/miniconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/yaml/composer.py\", line 133, in compose_mapping_node\n",
      "    item_value = self.compose_node(node, item_key)\n",
      "  File \"/Users/Natsume/miniconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/yaml/composer.py\", line 84, in compose_node\n",
      "    node = self.compose_mapping_node(anchor)\n",
      "  File \"/Users/Natsume/miniconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/yaml/composer.py\", line 133, in compose_mapping_node\n",
      "    item_value = self.compose_node(node, item_key)\n",
      "  File \"/Users/Natsume/miniconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/yaml/composer.py\", line 82, in compose_node\n",
      "    node = self.compose_sequence_node(anchor)\n",
      "  File \"/Users/Natsume/miniconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/yaml/composer.py\", line 111, in compose_sequence_node\n",
      "    node.value.append(self.compose_node(node, index))\n",
      "  File \"/Users/Natsume/miniconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/yaml/composer.py\", line 84, in compose_node\n",
      "    node = self.compose_mapping_node(anchor)\n",
      "  File \"/Users/Natsume/miniconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/yaml/composer.py\", line 133, in compose_mapping_node\n",
      "    item_value = self.compose_node(node, item_key)\n",
      "  File \"/Users/Natsume/miniconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/yaml/composer.py\", line 84, in compose_node\n",
      "    node = self.compose_mapping_node(anchor)\n",
      "  File \"/Users/Natsume/miniconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/yaml/composer.py\", line 133, in compose_mapping_node\n",
      "    item_value = self.compose_node(node, item_key)\n",
      "  File \"/Users/Natsume/miniconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/yaml/composer.py\", line 69, in compose_node\n",
      "    % anchor, event.start_mark)\n",
      "yaml.composer.ComposerError: found undefined alias 'cifar'\n",
      "  in \"<unicode string>\", line 24, column 13:\n",
      "            <<: *cifar\n",
      "                ^\n"
     ]
    }
   ],
   "source": [
    "!kur -v train dlnd_p2_fluid_sim.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dlnd_p2_dropout.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile dlnd_p2_dropout.yml\n",
    "\n",
    "---\n",
    "settings:\n",
    "\n",
    "  # Where to get the data\n",
    "  cifar: &cifar\n",
    "    url: \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "    checksum: \"6d958be074577803d12ecdefd02955f39262c83c16fe9348329d7fe0b5c001ce\"\n",
    "    path: \"~/kur\"           # if kur does not have normalization or one-hot-encoding, ##################\n",
    "                            # without normalization and one-hot-encoding, the performance will be hurt, right? ####\n",
    "  # Backend to use                                      \n",
    "  backend:\n",
    "    name: keras\n",
    "\n",
    "  # Hyperparameters\n",
    "#   cnn:\n",
    "#     kernels: 20\n",
    "#     size: [5, 5]\n",
    "#     strides: [2, 2]                \n",
    "\n",
    "#   pool:\n",
    "#     size: [2,2]\n",
    "#     strides: [2,2]\n",
    "#     type: max                         \n",
    "\n",
    "model:\n",
    "  - input: images                       \n",
    "  - convolution:                       \n",
    "      kernels: 20\n",
    "      size: [5, 5]\n",
    "      strides: [2,2]\n",
    "  - activation: relu\n",
    "  - pool:\n",
    "      size: [2,2]\n",
    "      strides: [2,2]\n",
    "      type: max\n",
    "  - flatten:\n",
    "  - dense: 15                           \n",
    "  - dropout: 0.25\n",
    "  - dense: 10\n",
    "  - activation: softmax\n",
    "    name: labels\n",
    "\n",
    "train:\n",
    "  data:\n",
    "    - cifar:\n",
    "        <<: *cifar\n",
    "        parts: [1]                      \n",
    "  provider: &provider\n",
    "    batch_size: 128\n",
    "    num_batches: 1000                        # use all batches\n",
    "    randomize: True\n",
    "  log: t1_dp0.25/cifar-log                   \n",
    "  epochs: 20\n",
    "  weights:\n",
    "    initial: t1_dp0.25/cifar.best.valid.w    \n",
    "    best: t1_dp0.25/cifar.best.train.w\n",
    "    last: t1_dp0.25/cifar.last.w\n",
    "\n",
    "  optimizer:\n",
    "    name: adam\n",
    "    learning_rate: 0.001\n",
    "        \n",
    "    #  hooks:\n",
    "    #    - plot: loss.png\n",
    "\n",
    "  hooks:\n",
    "    - plot:\n",
    "      loss_per_batch: t1_dp0.25/loss1.png\n",
    "      loss_per_time: t1_dp0.25/loss2.png\n",
    "      throughput_per_time: t1_dp0.25/loss3.png\n",
    "\n",
    "validate:\n",
    "  data:\n",
    "    - cifar:\n",
    "       <<: *cifar\n",
    "       parts: 5                          \n",
    "  provider: \n",
    "    <<: *provider       # can I do this? ##########\n",
    "    num_batches: 50     # override num_batches to be 50 while keep batch_size and randomize as 128 and True ####\n",
    "  weights: t1_dp0.25/cifar.best.valid.w\n",
    "\n",
    "test: &test\n",
    "  data:\n",
    "    - cifar:\n",
    "       <<: *cifar\n",
    "       parts: test\n",
    "  weights: t1_dp0.25/cifar.best.valid.w\n",
    "  provider: \n",
    "    <<: *provider\n",
    "    randomize: False\n",
    "\n",
    "\n",
    "evaluate:\n",
    "  <<: *test\n",
    "  destination: t1_dp0.25/cifar.results.pkl\n",
    "\n",
    "loss:\n",
    "  - target: labels                        # in the project: training loss and valid_accuracy are printed #############\n",
    "    name: categorical_crossentropy        # this should be a matter of personal taste, won't really affect anything##\n",
    "        \n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;37m[INFO 2017-03-02 21:17:35,801 kur.kurfile:699]\u001b[0m Parsing source: dlnd_p2_dropout.yml, included by top-level.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-02 21:17:35,819 kur.kurfile:82]\u001b[0m Parsing Kurfile...\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-02 21:17:35,848 kur.loggers.binary_logger:71]\u001b[0m Loading log data: t1_dp0.25/cifar-log\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Natsume/miniconda2/envs/dlnd-tf-lab/bin/kur\", line 11, in <module>\n",
      "    load_entry_point('kur', 'console_scripts', 'kur')()\n",
      "  File \"/Users/Natsume/Downloads/kur_road/kur/kur/__main__.py\", line 382, in main\n",
      "    sys.exit(args.func(args) or 0)\n",
      "  File \"/Users/Natsume/Downloads/kur_road/kur/kur/__main__.py\", line 61, in train\n",
      "    func = spec.get_training_function()\n",
      "  File \"/Users/Natsume/Downloads/kur_road/kur/kur/kurfile.py\", line 266, in get_training_function\n",
      "    for spec in training_hooks]\n",
      "  File \"/Users/Natsume/Downloads/kur_road/kur/kur/kurfile.py\", line 266, in <listcomp>\n",
      "    for spec in training_hooks]\n",
      "  File \"/Users/Natsume/Downloads/kur_road/kur/kur/model/hooks/hooks.py\", line 208, in from_specification\n",
      "    return target(**params)\n",
      "TypeError: __init__() missing 1 required positional argument: 'path'\n"
     ]
    }
   ],
   "source": [
    "!kur -v train dlnd_p2_dropout.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
